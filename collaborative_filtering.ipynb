{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>category</th>\n",
       "      <th>subcategory</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>url</th>\n",
       "      <th>title_entities</th>\n",
       "      <th>abstract_entities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>N88753</td>\n",
       "      <td>lifestyle</td>\n",
       "      <td>lifestyleroyals</td>\n",
       "      <td>The Brands Queen Elizabeth, Prince Charles, an...</td>\n",
       "      <td>Shop the notebooks, jackets, and more that the...</td>\n",
       "      <td>https://assets.msn.com/labs/mind/AAGH0ET.html</td>\n",
       "      <td>[{\"Label\": \"Prince Philip, Duke of Edinburgh\",...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>N45436</td>\n",
       "      <td>news</td>\n",
       "      <td>newsscienceandtechnology</td>\n",
       "      <td>Walmart Slashes Prices on Last-Generation iPads</td>\n",
       "      <td>Apple's new iPad releases bring big deals on l...</td>\n",
       "      <td>https://assets.msn.com/labs/mind/AABmf2I.html</td>\n",
       "      <td>[{\"Label\": \"IPad\", \"Type\": \"J\", \"WikidataId\": ...</td>\n",
       "      <td>[{\"Label\": \"IPad\", \"Type\": \"J\", \"WikidataId\": ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>N23144</td>\n",
       "      <td>health</td>\n",
       "      <td>weightloss</td>\n",
       "      <td>50 Worst Habits For Belly Fat</td>\n",
       "      <td>These seemingly harmless habits are holding yo...</td>\n",
       "      <td>https://assets.msn.com/labs/mind/AAB19MK.html</td>\n",
       "      <td>[{\"Label\": \"Adipose tissue\", \"Type\": \"C\", \"Wik...</td>\n",
       "      <td>[{\"Label\": \"Adipose tissue\", \"Type\": \"C\", \"Wik...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>N86255</td>\n",
       "      <td>health</td>\n",
       "      <td>medical</td>\n",
       "      <td>Dispose of unwanted prescription drugs during ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://assets.msn.com/labs/mind/AAISxPN.html</td>\n",
       "      <td>[{\"Label\": \"Drug Enforcement Administration\", ...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>N93187</td>\n",
       "      <td>news</td>\n",
       "      <td>newsworld</td>\n",
       "      <td>The Cost of Trump's Aid Freeze in the Trenches...</td>\n",
       "      <td>Lt. Ivan Molchanets peeked over a parapet of s...</td>\n",
       "      <td>https://assets.msn.com/labs/mind/AAJgNsz.html</td>\n",
       "      <td>[]</td>\n",
       "      <td>[{\"Label\": \"Ukraine\", \"Type\": \"G\", \"WikidataId...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  article_id   category               subcategory  \\\n",
       "0     N88753  lifestyle           lifestyleroyals   \n",
       "1     N45436       news  newsscienceandtechnology   \n",
       "2     N23144     health                weightloss   \n",
       "3     N86255     health                   medical   \n",
       "4     N93187       news                 newsworld   \n",
       "\n",
       "                                               title  \\\n",
       "0  The Brands Queen Elizabeth, Prince Charles, an...   \n",
       "1    Walmart Slashes Prices on Last-Generation iPads   \n",
       "2                      50 Worst Habits For Belly Fat   \n",
       "3  Dispose of unwanted prescription drugs during ...   \n",
       "4  The Cost of Trump's Aid Freeze in the Trenches...   \n",
       "\n",
       "                                            abstract  \\\n",
       "0  Shop the notebooks, jackets, and more that the...   \n",
       "1  Apple's new iPad releases bring big deals on l...   \n",
       "2  These seemingly harmless habits are holding yo...   \n",
       "3                                                NaN   \n",
       "4  Lt. Ivan Molchanets peeked over a parapet of s...   \n",
       "\n",
       "                                             url  \\\n",
       "0  https://assets.msn.com/labs/mind/AAGH0ET.html   \n",
       "1  https://assets.msn.com/labs/mind/AABmf2I.html   \n",
       "2  https://assets.msn.com/labs/mind/AAB19MK.html   \n",
       "3  https://assets.msn.com/labs/mind/AAISxPN.html   \n",
       "4  https://assets.msn.com/labs/mind/AAJgNsz.html   \n",
       "\n",
       "                                      title_entities  \\\n",
       "0  [{\"Label\": \"Prince Philip, Duke of Edinburgh\",...   \n",
       "1  [{\"Label\": \"IPad\", \"Type\": \"J\", \"WikidataId\": ...   \n",
       "2  [{\"Label\": \"Adipose tissue\", \"Type\": \"C\", \"Wik...   \n",
       "3  [{\"Label\": \"Drug Enforcement Administration\", ...   \n",
       "4                                                 []   \n",
       "\n",
       "                                   abstract_entities  \n",
       "0                                                 []  \n",
       "1  [{\"Label\": \"IPad\", \"Type\": \"J\", \"WikidataId\": ...  \n",
       "2  [{\"Label\": \"Adipose tissue\", \"Type\": \"C\", \"Wik...  \n",
       "3                                                 []  \n",
       "4  [{\"Label\": \"Ukraine\", \"Type\": \"G\", \"WikidataId...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_data = pd.read_csv(\"data/MINDlarge_train/news.tsv\", header=None, sep='\\t')\n",
    "news_data.columns = ['article_id', 'category', 'subcategory', 'title', 'abstract', 'url', 'title_entities', 'abstract_entities']\n",
    "\n",
    "news_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>impression_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>history</th>\n",
       "      <th>impressions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>U87243</td>\n",
       "      <td>11/10/2019 11:30:54 AM</td>\n",
       "      <td>N8668 N39081 N65259 N79529 N73408 N43615 N2937...</td>\n",
       "      <td>N78206-0 N26368-0 N7578-0 N58592-0 N19858-0 N5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>U598644</td>\n",
       "      <td>11/12/2019 1:45:29 PM</td>\n",
       "      <td>N56056 N8726 N70353 N67998 N83823 N111108 N107...</td>\n",
       "      <td>N47996-0 N82719-0 N117066-0 N8491-0 N123784-0 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>U532401</td>\n",
       "      <td>11/13/2019 11:23:03 AM</td>\n",
       "      <td>N128643 N87446 N122948 N9375 N82348 N129412 N5...</td>\n",
       "      <td>N103852-0 N53474-0 N127836-0 N47925-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>U593596</td>\n",
       "      <td>11/12/2019 12:24:09 PM</td>\n",
       "      <td>N31043 N39592 N4104 N8223 N114581 N92747 N1207...</td>\n",
       "      <td>N38902-0 N76434-0 N71593-0 N100073-0 N108736-0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>U239687</td>\n",
       "      <td>11/14/2019 8:03:01 PM</td>\n",
       "      <td>N65250 N122359 N71723 N53796 N41663 N41484 N11...</td>\n",
       "      <td>N76209-0 N48841-0 N67937-0 N62235-0 N6307-0 N3...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   impression_id  user_id               timestamp  \\\n",
       "0              1   U87243  11/10/2019 11:30:54 AM   \n",
       "1              2  U598644   11/12/2019 1:45:29 PM   \n",
       "2              3  U532401  11/13/2019 11:23:03 AM   \n",
       "3              4  U593596  11/12/2019 12:24:09 PM   \n",
       "4              5  U239687   11/14/2019 8:03:01 PM   \n",
       "\n",
       "                                             history  \\\n",
       "0  N8668 N39081 N65259 N79529 N73408 N43615 N2937...   \n",
       "1  N56056 N8726 N70353 N67998 N83823 N111108 N107...   \n",
       "2  N128643 N87446 N122948 N9375 N82348 N129412 N5...   \n",
       "3  N31043 N39592 N4104 N8223 N114581 N92747 N1207...   \n",
       "4  N65250 N122359 N71723 N53796 N41663 N41484 N11...   \n",
       "\n",
       "                                         impressions  \n",
       "0  N78206-0 N26368-0 N7578-0 N58592-0 N19858-0 N5...  \n",
       "1  N47996-0 N82719-0 N117066-0 N8491-0 N123784-0 ...  \n",
       "2              N103852-0 N53474-0 N127836-0 N47925-1  \n",
       "3  N38902-0 N76434-0 N71593-0 N100073-0 N108736-0...  \n",
       "4  N76209-0 N48841-0 N67937-0 N62235-0 N6307-0 N3...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "behavior_data = pd.read_csv(\"data/MINDlarge_train/behaviors.tsv\", header=None, sep='\\t')\n",
    "behavior_data.columns = ['impression_id', 'user_id', 'timestamp', 'history', 'impressions']\n",
    "\n",
    "behavior_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we have to create the user-item matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm import tqdm  # Optional for progress tracking\n",
    "\n",
    "def process_interactions_efficiently(behavior_data, batch_size=10000):\n",
    "    \"\"\"\n",
    "    Process behavior data to extract user-item interactions efficiently using batching.\n",
    "    \"\"\"\n",
    "    total_batches = (len(behavior_data) + batch_size - 1) // batch_size\n",
    "    interaction_dfs = []\n",
    "    \n",
    "    for batch_num in range(total_batches):\n",
    "        # Get a batch of the data\n",
    "        start_idx = batch_num * batch_size\n",
    "        end_idx = min((batch_num + 1) * batch_size, len(behavior_data))\n",
    "        batch = behavior_data.iloc[start_idx:end_idx]\n",
    "        \n",
    "        # Filter rows with valid history\n",
    "        valid_rows = batch[batch['history'].notna() & (batch['history'].str.strip() != '')]\n",
    "        \n",
    "        if len(valid_rows) > 0:\n",
    "            # Apply vectorized operations\n",
    "            temp_df = valid_rows[['user_id', 'history']].copy()\n",
    "            temp_df['article_id'] = temp_df['history'].str.split()\n",
    "            # Explode to create one row per user-article interaction\n",
    "            temp_df = temp_df.explode('article_id')\n",
    "            # Keep only the columns we need\n",
    "            temp_df = temp_df[['user_id', 'article_id']]\n",
    "            \n",
    "            interaction_dfs.append(temp_df)\n",
    "    \n",
    "    # Combine all batches into final dataframe\n",
    "    if interaction_dfs:\n",
    "        interactions_df = pd.concat(interaction_dfs, ignore_index=True)\n",
    "    else:\n",
    "        interactions_df = pd.DataFrame(columns=['user_id', 'article_id'])\n",
    "        \n",
    "    return interactions_df\n",
    "\n",
    "def create_sparse_matrices(interactions_df):\n",
    "    \"\"\"\n",
    "    Create sparse user-item matrix and compute sparse item similarity matrix.\n",
    "    Returns both matrices and mapping dictionaries.\n",
    "    \"\"\"\n",
    "    # Create mappings from IDs to indices\n",
    "    user_ids = interactions_df['user_id'].unique()\n",
    "    article_ids = interactions_df['article_id'].unique()\n",
    "    \n",
    "    user_id_to_idx = {id: i for i, id in enumerate(user_ids)}\n",
    "    article_id_to_idx = {id: i for i, id in enumerate(article_ids)}\n",
    "    \n",
    "    # Map the original IDs to matrix indices\n",
    "    user_indices = interactions_df['user_id'].map(user_id_to_idx).values\n",
    "    article_indices = interactions_df['article_id'].map(article_id_to_idx).values\n",
    "    \n",
    "    # Create interaction values (all 1s for implicit feedback)\n",
    "    interaction_values = np.ones(len(interactions_df), dtype=np.float32)\n",
    "    \n",
    "    # Create the sparse user-item matrix\n",
    "    sparse_user_item = csr_matrix(\n",
    "        (interaction_values, (user_indices, article_indices)),\n",
    "        shape=(len(user_ids), len(article_ids))\n",
    "    )\n",
    "    \n",
    "    # Create item-item similarity matrix (cosine similarity between items)\n",
    "    print(\"Computing item similarity matrix (this might take a while)...\")\n",
    "    sparse_item_similarity = cosine_similarity(sparse_user_item.T, dense_output=False)\n",
    "    \n",
    "    # Create reverse mappings to convert back to original IDs\n",
    "    idx_to_user_id = {i: id for id, i in user_id_to_idx.items()}\n",
    "    idx_to_article_id = {i: id for id, i in article_id_to_idx.items()}\n",
    "    \n",
    "    return (sparse_user_item, sparse_item_similarity, \n",
    "            user_id_to_idx, article_id_to_idx, \n",
    "            idx_to_user_id, idx_to_article_id)\n",
    "\n",
    "def get_item_recommendations_sparse(user_id, \n",
    "                                  sparse_user_item, \n",
    "                                  sparse_item_similarity,\n",
    "                                  user_id_to_idx, \n",
    "                                  article_id_to_idx,\n",
    "                                  idx_to_article_id,\n",
    "                                  top_n=5):\n",
    "    \"\"\"\n",
    "    Generate top-n item recommendations for a given user using sparse matrices.\n",
    "    \"\"\"\n",
    "    # Convert user_id to matrix index\n",
    "    if user_id not in user_id_to_idx:\n",
    "        return []\n",
    "    \n",
    "    user_idx = user_id_to_idx[user_id]\n",
    "    \n",
    "    # Get items the user has interacted with\n",
    "    user_interactions = sparse_user_item[user_idx].toarray().flatten()\n",
    "    interacted_item_indices = np.where(user_interactions > 0)[0]\n",
    "    \n",
    "    if len(interacted_item_indices) == 0:\n",
    "        return []  # User has no interactions, cannot recommend\n",
    "    \n",
    "    # Initialize scores array for all items\n",
    "    scores = np.zeros(sparse_item_similarity.shape[0])\n",
    "    \n",
    "    # For each item the user has interacted with\n",
    "    for item_idx in interacted_item_indices:\n",
    "        # Get similarity scores for this item with all other items\n",
    "        similarity_scores = sparse_item_similarity[item_idx].toarray().flatten()\n",
    "        # Add to accumulated scores\n",
    "        scores += similarity_scores\n",
    "    \n",
    "    # Set scores of items the user has already interacted with to -1 (to exclude them)\n",
    "    scores[interacted_item_indices] = -1\n",
    "    \n",
    "    # Get indices of top_n items with highest scores\n",
    "    recommended_indices = np.argsort(scores)[::-1][:top_n]\n",
    "    \n",
    "    # Convert indices back to article IDs\n",
    "    recommended_articles = [idx_to_article_id[idx] for idx in recommended_indices if scores[idx] > 0]\n",
    "    \n",
    "    return recommended_articles\n",
    "\n",
    "def batch_generate_recommendations(user_ids, \n",
    "                                  sparse_user_item, \n",
    "                                  sparse_item_similarity,\n",
    "                                  user_id_to_idx, \n",
    "                                  article_id_to_idx,\n",
    "                                  idx_to_article_id,\n",
    "                                  top_n=5):\n",
    "    \"\"\"\n",
    "    Generate recommendations for multiple users efficiently.\n",
    "    \"\"\"\n",
    "    recommendations = {}\n",
    "    \n",
    "    for user_id in tqdm(user_ids, desc=\"Generating recommendations\"):\n",
    "        recs = get_item_recommendations_sparse(\n",
    "            user_id, \n",
    "            sparse_user_item, \n",
    "            sparse_item_similarity,\n",
    "            user_id_to_idx, \n",
    "            article_id_to_idx,\n",
    "            idx_to_article_id,\n",
    "            top_n=top_n\n",
    "        )\n",
    "        recommendations[user_id] = recs\n",
    "        \n",
    "    return recommendations\n",
    "\n",
    "\n",
    "def collaborative_filtering_pipeline(behavior_data, top_n=5, sample_users=None):\n",
    "    \"\"\"\n",
    "    Complete pipeline for collaborative filtering.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    behavior_data : DataFrame\n",
    "        DataFrame containing user_id and history columns\n",
    "    top_n : int\n",
    "        Number of recommendations per user\n",
    "    sample_users : list or None\n",
    "        List of specific user_ids to generate recommendations for,\n",
    "        or None to use all users\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple: (recommendations, sparse_user_item, sparse_item_similarity, \n",
    "            user_id_to_idx, article_id_to_idx, idx_to_user_id, idx_to_article_id)\n",
    "    \"\"\"\n",
    "    print(\"Processing interactions...\")\n",
    "    interactions_df = process_interactions_efficiently(behavior_data)\n",
    "    \n",
    "    print(\"Creating sparse matrices...\")\n",
    "    (sparse_user_item, sparse_item_similarity, \n",
    "     user_id_to_idx, article_id_to_idx, \n",
    "     idx_to_user_id, idx_to_article_id) = create_sparse_matrices(interactions_df)\n",
    "    \n",
    "    print(f\"User-item matrix shape: {sparse_user_item.shape}\")\n",
    "    print(f\"Density: {sparse_user_item.nnz / (sparse_user_item.shape[0] * sparse_user_item.shape[1]):.6f}\")\n",
    "    \n",
    "    if sample_users is None:\n",
    "        # Use all users (or first 100 for demonstration)\n",
    "        sample_users = list(user_id_to_idx.keys())[:500]  # Limit for demonstration\n",
    "    \n",
    "    print(f\"Generating recommendations for {len(sample_users)} users...\")\n",
    "    recommendations = batch_generate_recommendations(\n",
    "        sample_users,\n",
    "        sparse_user_item, \n",
    "        sparse_item_similarity,\n",
    "        user_id_to_idx, \n",
    "        article_id_to_idx,\n",
    "        idx_to_article_id,\n",
    "        top_n=top_n\n",
    "    )\n",
    "    \n",
    "    # Return all necessary variables for evaluation\n",
    "    return (recommendations, sparse_user_item, sparse_item_similarity, \n",
    "            user_id_to_idx, article_id_to_idx, idx_to_user_id, idx_to_article_id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>impression_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>history</th>\n",
       "      <th>impressions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>U134050</td>\n",
       "      <td>11/15/2019 8:55:22 AM</td>\n",
       "      <td>N12246 N128820 N119226 N4065 N67770 N33446 N10...</td>\n",
       "      <td>N91737-0 N30206-0 N54368-0 N117802-0 N18190-0 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>U254959</td>\n",
       "      <td>11/15/2019 11:42:35 AM</td>\n",
       "      <td>N34011 N9375 N67397 N7936 N118985 N109453 N103...</td>\n",
       "      <td>N119999-0 N24958-0 N104054-0 N33901-0 N9250-0 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>U499841</td>\n",
       "      <td>11/15/2019 9:08:21 AM</td>\n",
       "      <td>N63858 N26834 N6379 N85484 N15229 N65119 N1047...</td>\n",
       "      <td>N18190-0 N89764-0 N91737-0 N54368-0 N49978-1 N...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>U107107</td>\n",
       "      <td>11/15/2019 5:50:31 AM</td>\n",
       "      <td>N12959 N8085 N18389 N3758 N9740 N90543 N129790...</td>\n",
       "      <td>N122944-1 N18190-0 N55801-0 N59297-0 N128045-0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>U492344</td>\n",
       "      <td>11/15/2019 5:02:25 AM</td>\n",
       "      <td>N109183 N48453 N85005 N45706 N98923 N46069 N35...</td>\n",
       "      <td>N64785-0 N82503-0 N32993-0 N122944-0 N29160-0 ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   impression_id  user_id               timestamp  \\\n",
       "0              1  U134050   11/15/2019 8:55:22 AM   \n",
       "1              2  U254959  11/15/2019 11:42:35 AM   \n",
       "2              3  U499841   11/15/2019 9:08:21 AM   \n",
       "3              4  U107107   11/15/2019 5:50:31 AM   \n",
       "4              5  U492344   11/15/2019 5:02:25 AM   \n",
       "\n",
       "                                             history  \\\n",
       "0  N12246 N128820 N119226 N4065 N67770 N33446 N10...   \n",
       "1  N34011 N9375 N67397 N7936 N118985 N109453 N103...   \n",
       "2  N63858 N26834 N6379 N85484 N15229 N65119 N1047...   \n",
       "3  N12959 N8085 N18389 N3758 N9740 N90543 N129790...   \n",
       "4  N109183 N48453 N85005 N45706 N98923 N46069 N35...   \n",
       "\n",
       "                                         impressions  \n",
       "0  N91737-0 N30206-0 N54368-0 N117802-0 N18190-0 ...  \n",
       "1  N119999-0 N24958-0 N104054-0 N33901-0 N9250-0 ...  \n",
       "2  N18190-0 N89764-0 N91737-0 N54368-0 N49978-1 N...  \n",
       "3  N122944-1 N18190-0 N55801-0 N59297-0 N128045-0...  \n",
       "4  N64785-0 N82503-0 N32993-0 N122944-0 N29160-0 ...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_behavior_data = pd.read_csv(\"data/MINDlarge_dev/behaviors.tsv\", header=None, sep='\\t')\n",
    "test_behavior_data.columns = ['impression_id', 'user_id', 'timestamp', 'history', 'impressions']\n",
    "\n",
    "test_behavior_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing interactions...\n",
      "Creating sparse matrices...\n",
      "Computing item similarity matrix (this might take a while)...\n",
      "User-item matrix shape: (698365, 79546)\n",
      "Density: 0.000237\n",
      "Generating recommendations for 500 users...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating recommendations: 100%|██████████| 500/500 [00:05<00:00, 95.21it/s] \n",
      "100%|██████████| 30000/30000 [04:01<00:00, 124.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 1128901 predictions\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>impression_id</th>\n",
       "      <th>news_id</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>181577</td>\n",
       "      <td>N83707</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>181577</td>\n",
       "      <td>N26122</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>181577</td>\n",
       "      <td>N32993</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>181577</td>\n",
       "      <td>N80770</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>181577</td>\n",
       "      <td>N86609</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  impression_id news_id  score\n",
       "0        181577  N83707    0.5\n",
       "1        181577  N26122    0.5\n",
       "2        181577  N32993    0.5\n",
       "3        181577  N80770    0.5\n",
       "4        181577  N86609    0.5"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "def generate_mind_predictions(test_behavior_data, \n",
    "                                         sparse_user_item, \n",
    "                                         sparse_item_similarity,\n",
    "                                         user_id_to_idx, \n",
    "                                         article_id_to_idx,\n",
    "                                         idx_to_article_id):\n",
    "    \"\"\"\n",
    "    Generate prediction scores with compatible datatypes for evaluation.\n",
    "    \"\"\"\n",
    "    predictions = []\n",
    "    \n",
    "    for idx, row in tqdm(test_behavior_data.iterrows(), total=len(test_behavior_data)):\n",
    "        # Get impression_id (use row index if not in columns)\n",
    "        impression_id = row.get('impression_id', idx)\n",
    "        user_id = row['user_id']\n",
    "        \n",
    "        if pd.isna(row['impressions']):\n",
    "            continue\n",
    "            \n",
    "        # Parse impression articles\n",
    "        for imp in row['impressions'].split():\n",
    "            parts = imp.split('-')\n",
    "            article_id = parts[0]\n",
    "            \n",
    "            # Calculate score (using default 0.5 for simplicity in this example)\n",
    "            score = 0.5\n",
    "            \n",
    "            # If user and article exist in our model, calculate real score\n",
    "            if user_id in user_id_to_idx and article_id in article_id_to_idx:\n",
    "                user_idx = user_id_to_idx[user_id]\n",
    "                article_idx = article_id_to_idx[article_id]\n",
    "                \n",
    "                # Get user's interaction history\n",
    "                user_interactions = sparse_user_item[user_idx].toarray().flatten()\n",
    "                interacted_item_indices = np.where(user_interactions > 0)[0]\n",
    "                \n",
    "                if len(interacted_item_indices) > 0:\n",
    "                    # Calculate similarity-based score\n",
    "                    total_sim = 0\n",
    "                    \n",
    "                    for item_idx in interacted_item_indices:\n",
    "                        # Get similarity safely\n",
    "                        sim_matrix = sparse_item_similarity[item_idx].toarray().flatten()\n",
    "                        if article_idx < len(sim_matrix):\n",
    "                            total_sim += sim_matrix[article_idx]\n",
    "                    \n",
    "                    # Normalize to 0-1 range (simple approach)\n",
    "                    # This is a simplified normalization - adjust if needed\n",
    "                    score = min(1.0, max(0.0, total_sim / max(1, len(interacted_item_indices))))\n",
    "            \n",
    "            # Append to predictions list\n",
    "            predictions.append({\n",
    "                'impression_id': impression_id,\n",
    "                'news_id': article_id,\n",
    "                'score': float(score)  # Ensure float\n",
    "            })\n",
    "    \n",
    "    # Create DataFrame with explicit dtypes\n",
    "    predictions_df = pd.DataFrame(predictions)\n",
    "    \n",
    "    # Ensure correct datatypes for Polars conversion\n",
    "    predictions_df['impression_id'] = predictions_df['impression_id'].astype(str)\n",
    "    predictions_df['news_id'] = predictions_df['news_id'].astype(str)\n",
    "    predictions_df['score'] = predictions_df['score'].astype(float)\n",
    "    \n",
    "    \n",
    "    \n",
    "    return predictions_df\n",
    "\n",
    "# Example usage:\n",
    "# This code should be placed after running the collaborative_filtering_pipeline\n",
    "\n",
    "(recommendations, sparse_user_item, sparse_item_similarity, \n",
    "            user_id_to_idx, article_id_to_idx, idx_to_user_id, idx_to_article_id) = collaborative_filtering_pipeline(behavior_data, top_n=5)\n",
    "\n",
    "sample_size = 30000  # Adjust this based on your needs\n",
    "test_sample = test_behavior_data.sample(n=sample_size, random_state=42)\n",
    "\n",
    "# Generate predictions for test data\n",
    "predictions_df = generate_mind_predictions(\n",
    "    test_sample,\n",
    "    sparse_user_item, \n",
    "    sparse_item_similarity,\n",
    "    user_id_to_idx, \n",
    "    article_id_to_idx,\n",
    "    idx_to_article_id\n",
    ")\n",
    "\n",
    "# You can then save predictions to a CSV file for submission or evaluation\n",
    "predictions_df.to_csv('mind_predictions.csv', index=False)\n",
    "\n",
    "print(f\"Generated {len(predictions_df)} predictions\")\n",
    "predictions_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_df['impression_id'] = predictions_df['impression_id'].astype('int64')  # i64\n",
    "predictions_df['news_id'] = predictions_df['news_id'].astype(str)                 # str\n",
    "predictions_df['score'] = predictions_df['score'].astype('float64')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>impression_id</th>\n",
       "      <th>news_id</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>181577</td>\n",
       "      <td>N83707</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>181577</td>\n",
       "      <td>N26122</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>181577</td>\n",
       "      <td>N32993</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>181577</td>\n",
       "      <td>N80770</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>181577</td>\n",
       "      <td>N86609</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   impression_id news_id  score\n",
       "0         181577  N83707    0.5\n",
       "1         181577  N26122    0.5\n",
       "2         181577  N32993    0.5\n",
       "3         181577  N80770    0.5\n",
       "4         181577  N86609    0.5"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'auc': np.float64(0.5122522263430606),\n",
       " 'mrr': np.float64(0.24543784074002772),\n",
       " 'ndcg@5': np.float64(0.2479285666195759),\n",
       " 'ndcg@10': np.float64(0.31242379386645075)}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import lib.eval as eval\n",
    "import importlib\n",
    "\n",
    "importlib.reload(eval)\n",
    "\n",
    "eval.evaluate_mind_predictions(predictions_df, test_behavior_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collaborative filtering with ALS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TG-IDF implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing interactions...\n",
      "Creating TF-IDF matrices...\n",
      "Computing item similarity matrix with TF-IDF weighting (this might take a while)...\n",
      "User-item matrix shape: (698365, 79546)\n",
      "Density: 0.000237\n",
      "Generating predictions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30000/30000 [01:10<00:00, 423.50it/s]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix, diags\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm import tqdm\n",
    "\n",
    "def process_interactions_efficiently(behavior_data, batch_size=10000):\n",
    "    \"\"\"\n",
    "    Process behavior data to extract user-item interactions efficiently using batching.\n",
    "    \"\"\"\n",
    "    total_batches = (len(behavior_data) + batch_size - 1) // batch_size\n",
    "    interaction_dfs = []\n",
    "    \n",
    "    for batch_num in range(total_batches):\n",
    "        # Get a batch of the data\n",
    "        start_idx = batch_num * batch_size\n",
    "        end_idx = min((batch_num + 1) * batch_size, len(behavior_data))\n",
    "        batch = behavior_data.iloc[start_idx:end_idx]\n",
    "        \n",
    "        # Filter rows with valid history\n",
    "        valid_rows = batch[batch['history'].notna() & (batch['history'].str.strip() != '')]\n",
    "        \n",
    "        if len(valid_rows) > 0:\n",
    "            # Apply vectorized operations\n",
    "            temp_df = valid_rows[['user_id', 'history']].copy()\n",
    "            temp_df['article_id'] = temp_df['history'].str.split()\n",
    "            # Explode to create one row per user-article interaction\n",
    "            temp_df = temp_df.explode('article_id')\n",
    "            # Keep only the columns we need\n",
    "            temp_df = temp_df[['user_id', 'article_id']]\n",
    "            \n",
    "            interaction_dfs.append(temp_df)\n",
    "    \n",
    "    # Combine all batches into final dataframe\n",
    "    if interaction_dfs:\n",
    "        interactions_df = pd.concat(interaction_dfs, ignore_index=True)\n",
    "    else:\n",
    "        interactions_df = pd.DataFrame(columns=['user_id', 'article_id'])\n",
    "        \n",
    "    return interactions_df\n",
    "\n",
    "def create_tfidf_matrices(interactions_df):\n",
    "    \"\"\"\n",
    "    Create sparse user-item matrix with TF-IDF weighting and compute item similarity matrix.\n",
    "    \"\"\"\n",
    "    # Create mappings from IDs to indices\n",
    "    user_ids = interactions_df['user_id'].unique()\n",
    "    article_ids = interactions_df['article_id'].unique()\n",
    "    \n",
    "    user_id_to_idx = {id: i for i, id in enumerate(user_ids)}\n",
    "    article_id_to_idx = {id: i for i, id in enumerate(article_ids)}\n",
    "    \n",
    "    # Map the original IDs to matrix indices\n",
    "    user_indices = interactions_df['user_id'].map(user_id_to_idx).values\n",
    "    article_indices = interactions_df['article_id'].map(article_id_to_idx).values\n",
    "    \n",
    "    # Create interaction values (all 1s for implicit feedback)\n",
    "    interaction_values = np.ones(len(interactions_df), dtype=np.float32)\n",
    "    \n",
    "    # Create the basic sparse user-item matrix\n",
    "    sparse_user_item = csr_matrix(\n",
    "        (interaction_values, (user_indices, article_indices)),\n",
    "        shape=(len(user_ids), len(article_ids))\n",
    "    )\n",
    "    \n",
    "    # Compute TF (Term Frequency) - normalized by user interaction count\n",
    "    # This represents how important an article is to a user\n",
    "    user_interaction_counts = np.array(sparse_user_item.sum(axis=1)).flatten()\n",
    "    user_interaction_counts[user_interaction_counts == 0] = 1.0  # Avoid division by zero\n",
    "    tf_diag = diags(1.0 / user_interaction_counts, 0)\n",
    "    tf_matrix = tf_diag.dot(sparse_user_item)\n",
    "    \n",
    "    # Compute IDF (Inverse Document Frequency)\n",
    "    # This downweights popular articles that many users have interacted with\n",
    "    item_interaction_counts = np.array(sparse_user_item.sum(axis=0)).flatten()\n",
    "    n_users = sparse_user_item.shape[0]\n",
    "    idf = np.log(n_users / (item_interaction_counts + 1.0))\n",
    "    idf_diag = diags(idf, 0)\n",
    "    \n",
    "    # Create TF-IDF weighted user-item matrix\n",
    "    tfidf_matrix = tf_matrix.dot(idf_diag)\n",
    "    \n",
    "    # Compute item popularity (for potential use in recommendations)\n",
    "    item_popularity = item_interaction_counts / np.sum(item_interaction_counts)\n",
    "    \n",
    "    # Create item-item similarity matrix using the TF-IDF weighted matrix\n",
    "    print(\"Computing item similarity matrix with TF-IDF weighting (this might take a while)...\")\n",
    "    sparse_item_similarity = cosine_similarity(tfidf_matrix.T, dense_output=False)\n",
    "    \n",
    "    # Create reverse mappings to convert back to original IDs\n",
    "    idx_to_user_id = {i: id for id, i in user_id_to_idx.items()}\n",
    "    idx_to_article_id = {i: id for id, i in article_id_to_idx.items()}\n",
    "    \n",
    "    return (tfidf_matrix, sparse_item_similarity, \n",
    "            user_id_to_idx, article_id_to_idx, \n",
    "            idx_to_user_id, idx_to_article_id,\n",
    "            item_popularity)\n",
    "\n",
    "def get_item_recommendations_with_tfidf(user_id, \n",
    "                                     tfidf_matrix, \n",
    "                                     sparse_item_similarity,\n",
    "                                     user_id_to_idx, \n",
    "                                     article_id_to_idx,\n",
    "                                     idx_to_article_id,\n",
    "                                     item_popularity,\n",
    "                                     top_n=5):\n",
    "    \"\"\"\n",
    "    Generate top-n item recommendations for a given user using TF-IDF weighted similarity.\n",
    "    \"\"\"\n",
    "    # Convert user_id to matrix index\n",
    "    if user_id not in user_id_to_idx:\n",
    "        return []\n",
    "    \n",
    "    user_idx = user_id_to_idx[user_id]\n",
    "    \n",
    "    # Get items the user has interacted with (from TF-IDF matrix)\n",
    "    user_interactions = tfidf_matrix[user_idx].toarray().flatten()\n",
    "    interacted_item_indices = np.where(user_interactions > 0)[0]\n",
    "    \n",
    "    if len(interacted_item_indices) == 0:\n",
    "        return []  # User has no interactions, cannot recommend\n",
    "    \n",
    "    # Initialize scores array for all items\n",
    "    scores = np.zeros(sparse_item_similarity.shape[0])\n",
    "    \n",
    "    # For each item the user has interacted with\n",
    "    for item_idx in interacted_item_indices:\n",
    "        # Get similarity scores for this item with all other items\n",
    "        similarity_scores = sparse_item_similarity[item_idx].toarray().flatten()\n",
    "        \n",
    "        # Weight by the user's TF-IDF score for this item\n",
    "        # This gives more importance to items that are more significant to the user\n",
    "        item_weight = user_interactions[item_idx]\n",
    "        scores += similarity_scores * item_weight\n",
    "    \n",
    "    # Set scores of items the user has already interacted with to -1 (to exclude them)\n",
    "    scores[interacted_item_indices] = -1\n",
    "    \n",
    "    # Get indices of top_n items with highest scores\n",
    "    recommended_indices = np.argsort(scores)[::-1][:top_n]\n",
    "    \n",
    "    # Convert indices back to article IDs\n",
    "    recommended_articles = [idx_to_article_id[idx] for idx in recommended_indices if scores[idx] > 0]\n",
    "    \n",
    "    return recommended_articles\n",
    "\n",
    "def generate_mind_predictions_tfidf(test_behavior_data,\n",
    "                                  tfidf_matrix,\n",
    "                                  sparse_item_similarity,\n",
    "                                  user_id_to_idx,\n",
    "                                  article_id_to_idx,\n",
    "                                  idx_to_article_id,\n",
    "                                  item_popularity):\n",
    "    \"\"\"\n",
    "    Generate prediction scores using TF-IDF weighted collaborative filtering.\n",
    "    \"\"\"\n",
    "    predictions = []\n",
    "    \n",
    "    for idx, row in tqdm(test_behavior_data.iterrows(), total=len(test_behavior_data)):\n",
    "        # Get impression_id (use row index if not in columns)\n",
    "        impression_id = row.get('impression_id', idx)\n",
    "        user_id = row['user_id']\n",
    "        \n",
    "        if pd.isna(row['impressions']):\n",
    "            continue\n",
    "            \n",
    "        # Parse impression articles\n",
    "        for imp in row['impressions'].split():\n",
    "            parts = imp.split('-')\n",
    "            article_id = parts[0]\n",
    "            \n",
    "            # Default score\n",
    "            score = 0.5\n",
    "            \n",
    "            # If user and article exist in our model, calculate real score\n",
    "            if user_id in user_id_to_idx and article_id in article_id_to_idx:\n",
    "                user_idx = user_id_to_idx[user_id]\n",
    "                article_idx = article_id_to_idx[article_id]\n",
    "                \n",
    "                # Get user's interaction history from TF-IDF matrix\n",
    "                user_interactions = tfidf_matrix[user_idx].toarray().flatten()\n",
    "                interacted_item_indices = np.where(user_interactions > 0)[0]\n",
    "                \n",
    "                if len(interacted_item_indices) > 0:\n",
    "                    # Calculate TF-IDF weighted similarity score\n",
    "                    total_sim = 0\n",
    "                    weight_sum = 0\n",
    "                    \n",
    "                    for item_idx in interacted_item_indices:\n",
    "                        # Get similarity between this item and target article\n",
    "                        sim_score = sparse_item_similarity[item_idx, article_idx]\n",
    "                        if sim_score:\n",
    "                            # Weight by the user's TF-IDF score for this item\n",
    "                            item_weight = user_interactions[item_idx]\n",
    "                            total_sim += sim_score * item_weight\n",
    "                            weight_sum += item_weight\n",
    "                    \n",
    "                    # Compute final score\n",
    "                    if weight_sum > 0:\n",
    "                        raw_score = total_sim / weight_sum\n",
    "                        # Scale to [0, 1] range\n",
    "                        score = min(1.0, max(0.0, raw_score))\n",
    "                    elif article_idx < len(item_popularity):\n",
    "                        # Fallback to item popularity\n",
    "                        score = 0.4 + 0.2 * item_popularity[article_idx] / max(item_popularity)\n",
    "            \n",
    "            # Append to predictions list\n",
    "            predictions.append({\n",
    "                'impression_id': impression_id,\n",
    "                'news_id': article_id,\n",
    "                'score': float(score)\n",
    "            })\n",
    "    \n",
    "    # Create DataFrame with explicit dtypes\n",
    "    predictions_df = pd.DataFrame(predictions)\n",
    "    \n",
    "    # Ensure correct datatypes for evaluation\n",
    "    try:\n",
    "        predictions_df['impression_id'] = predictions_df['impression_id'].astype('int64')\n",
    "    except:\n",
    "        predictions_df['impression_id'] = predictions_df['impression_id'].astype(str)\n",
    "    \n",
    "    predictions_df['news_id'] = predictions_df['news_id'].astype(str)\n",
    "    predictions_df['score'] = predictions_df['score'].astype(float)\n",
    "    \n",
    "    return predictions_df\n",
    "\n",
    "def tfidf_collaborative_filtering_pipeline(behavior_data, test_behavior_data, top_n=5):\n",
    "    \"\"\"\n",
    "    Complete pipeline for TF-IDF enhanced collaborative filtering.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    behavior_data : DataFrame\n",
    "        Training data containing user_id and history columns\n",
    "    test_behavior_data : DataFrame\n",
    "        Test data for generating predictions\n",
    "    top_n : int\n",
    "        Number of recommendations per user\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    predictions_df : DataFrame\n",
    "        DataFrame with impression_id, news_id, and score columns\n",
    "    \"\"\"\n",
    "    print(\"Processing interactions...\")\n",
    "    interactions_df = process_interactions_efficiently(behavior_data)\n",
    "    \n",
    "    print(\"Creating TF-IDF matrices...\")\n",
    "    (tfidf_matrix, sparse_item_similarity, \n",
    "     user_id_to_idx, article_id_to_idx, \n",
    "     idx_to_user_id, idx_to_article_id,\n",
    "     item_popularity) = create_tfidf_matrices(interactions_df)\n",
    "    \n",
    "    print(f\"User-item matrix shape: {tfidf_matrix.shape}\")\n",
    "    print(f\"Density: {tfidf_matrix.nnz / (tfidf_matrix.shape[0] * tfidf_matrix.shape[1]):.6f}\")\n",
    "    \n",
    "    print(\"Generating predictions...\")\n",
    "    predictions_df = generate_mind_predictions_tfidf(\n",
    "        test_behavior_data,\n",
    "        tfidf_matrix, \n",
    "        sparse_item_similarity,\n",
    "        user_id_to_idx, \n",
    "        article_id_to_idx,\n",
    "        idx_to_article_id,\n",
    "        item_popularity\n",
    "    )\n",
    "    \n",
    "    return predictions_df\n",
    "\n",
    "# Example usage:\n",
    "sample_size = 30000  # Adjust based on your needs\n",
    "test_sample = test_behavior_data.sample(n=sample_size, random_state=42)\n",
    "predictions_df = tfidf_collaborative_filtering_pipeline(behavior_data, test_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'auc': np.float64(0.5123415509984646),\n",
       " 'mrr': np.float64(0.24546161311513062),\n",
       " 'ndcg@5': np.float64(0.24797189151305932),\n",
       " 'ndcg@10': np.float64(0.31238071121777455)}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "importlib.reload(eval)\n",
    "\n",
    "eval.evaluate_mind_predictions(predictions_df, test_behavior_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New METHOD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(eval)\n",
    "\n",
    "eval.evaluate_mind_predictions(predictions_df, test_behavior_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting optimized item-based collaborative filtering pipeline...\n",
      "Processing interactions...\n",
      "Processed 12695274 interactions from 698365 users\n",
      "Creating optimized item similarity matrix...\n",
      "Computing item similarity (this might take a minute or two)...\n",
      "Matrix creation completed in 9.26 seconds\n",
      "Generating predictions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30000/30000 [00:41<00:00, 723.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction generation completed in 41.93 seconds\n",
      "Pipeline completed in 152.11 seconds\n",
      "Generated 1128901 predictions\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix, diags\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "def process_interactions_efficiently(behavior_data, max_articles_per_user=20, batch_size=10000):\n",
    "    \"\"\"\n",
    "    Process behavior data to extract user-item interactions efficiently.\n",
    "    Limits the number of articles per user for efficiency and to focus on recent behavior.\n",
    "    \"\"\"\n",
    "    print(\"Processing interactions...\")\n",
    "    total_batches = (len(behavior_data) + batch_size - 1) // batch_size\n",
    "    interaction_dfs = []\n",
    "    \n",
    "    for batch_num in range(total_batches):\n",
    "        # Get a batch of the data\n",
    "        start_idx = batch_num * batch_size\n",
    "        end_idx = min((batch_num + 1) * batch_size, len(behavior_data))\n",
    "        batch = behavior_data.iloc[start_idx:end_idx]\n",
    "        \n",
    "        # Filter rows with valid history\n",
    "        valid_rows = batch[batch['history'].notna() & (batch['history'].str.strip() != '')]\n",
    "        \n",
    "        if len(valid_rows) > 0:\n",
    "            user_interactions = []\n",
    "            \n",
    "            for _, row in valid_rows.iterrows():\n",
    "                user_id = row['user_id']\n",
    "                history = row['history'].split()\n",
    "                \n",
    "                # Take only the most recent articles if there are too many\n",
    "                if len(history) > max_articles_per_user:\n",
    "                    history = history[-max_articles_per_user:]\n",
    "                \n",
    "                # Assign higher weights to more recent articles\n",
    "                for i, article_id in enumerate(history):\n",
    "                    # Position-based weight (recent items get higher weight)\n",
    "                    position = i / len(history)\n",
    "                    weight = 0.5 + 0.5 * position  # Ranges from 0.5 to 1.0\n",
    "                    \n",
    "                    user_interactions.append({\n",
    "                        'user_id': user_id,\n",
    "                        'article_id': article_id,\n",
    "                        'weight': weight\n",
    "                    })\n",
    "            \n",
    "            if user_interactions:\n",
    "                interaction_df = pd.DataFrame(user_interactions)\n",
    "                interaction_dfs.append(interaction_df)\n",
    "    \n",
    "    # Combine all batches into final dataframe\n",
    "    if interaction_dfs:\n",
    "        interactions_df = pd.concat(interaction_dfs, ignore_index=True)\n",
    "        \n",
    "        # Aggregate weights for duplicate user-item pairs\n",
    "        interactions_df = interactions_df.groupby(['user_id', 'article_id'])['weight'].sum().reset_index()\n",
    "    else:\n",
    "        interactions_df = pd.DataFrame(columns=['user_id', 'article_id', 'weight'])\n",
    "        \n",
    "    print(f\"Processed {len(interactions_df)} interactions from {interactions_df['user_id'].nunique()} users\")\n",
    "    return interactions_df\n",
    "\n",
    "def create_optimized_item_similarity(interactions_df, max_items=30000):\n",
    "    \"\"\"\n",
    "    Create optimized item similarity matrix focusing only on the most popular items.\n",
    "    \"\"\"\n",
    "    print(\"Creating optimized item similarity matrix...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Limit to top items if needed\n",
    "    if max_items and len(interactions_df['article_id'].unique()) > max_items:\n",
    "        item_counts = interactions_df.groupby('article_id')['weight'].sum().reset_index()\n",
    "        top_items = item_counts.sort_values('weight', ascending=False).head(max_items)['article_id'].values\n",
    "        interactions_df = interactions_df[interactions_df['article_id'].isin(top_items)]\n",
    "        print(f\"Limited to top {max_items} items\")\n",
    "    \n",
    "    # Create mappings from IDs to indices\n",
    "    user_ids = interactions_df['user_id'].unique()\n",
    "    article_ids = interactions_df['article_id'].unique()\n",
    "    \n",
    "    user_id_to_idx = {id: i for i, id in enumerate(user_ids)}\n",
    "    article_id_to_idx = {id: i for i, id in enumerate(article_ids)}\n",
    "    \n",
    "    # Map the original IDs to matrix indices\n",
    "    user_indices = interactions_df['user_id'].map(user_id_to_idx).values\n",
    "    article_indices = interactions_df['article_id'].map(article_id_to_idx).values\n",
    "    \n",
    "    # Use weights from interactions\n",
    "    interaction_values = interactions_df['weight'].values\n",
    "    \n",
    "    # Create the sparse user-item matrix\n",
    "    sparse_user_item = csr_matrix(\n",
    "        (interaction_values, (user_indices, article_indices)),\n",
    "        shape=(len(user_ids), len(article_ids))\n",
    "    )\n",
    "    \n",
    "    # Calculate item popularity\n",
    "    item_popularity = np.array(sparse_user_item.sum(axis=0)).flatten()\n",
    "    item_popularity = item_popularity / (np.sum(item_popularity) or 1)\n",
    "    \n",
    "    # Apply IDF weighting to reduce popularity bias\n",
    "    # This gives more weight to rare items and less to common ones\n",
    "    n_users = sparse_user_item.shape[0]\n",
    "    item_user_counts = np.array((sparse_user_item > 0).sum(axis=0)).flatten()\n",
    "    idf = np.log(n_users / (item_user_counts + 1))\n",
    "    idf_diag = diags(idf, 0)\n",
    "    \n",
    "    # Apply IDF weighting to user-item matrix\n",
    "    weighted_user_item = sparse_user_item.dot(idf_diag)\n",
    "    \n",
    "    # Normalize the user vectors for better similarity calculation\n",
    "    user_norms = np.sqrt(weighted_user_item.power(2).sum(axis=1))\n",
    "    user_norms[user_norms == 0] = 1  # Avoid division by zero\n",
    "    row_normalizer = diags(1.0 / user_norms.A.flatten(), 0)\n",
    "    normalized_matrix = row_normalizer.dot(weighted_user_item)\n",
    "    \n",
    "    # Compute item similarity\n",
    "    print(\"Computing item similarity (this might take a minute or two)...\")\n",
    "    item_similarity = cosine_similarity(normalized_matrix.T, dense_output=False)\n",
    "    \n",
    "    # Create reverse mappings\n",
    "    idx_to_user_id = {i: id for id, i in user_id_to_idx.items()}\n",
    "    idx_to_article_id = {i: id for id, i in article_id_to_idx.items()}\n",
    "    \n",
    "    end_time = time.time()\n",
    "    print(f\"Matrix creation completed in {end_time - start_time:.2f} seconds\")\n",
    "    \n",
    "    return (sparse_user_item, item_similarity, \n",
    "            user_id_to_idx, article_id_to_idx, \n",
    "            idx_to_user_id, idx_to_article_id,\n",
    "            item_popularity)\n",
    "\n",
    "def generate_optimized_predictions(test_behavior_data,\n",
    "                                 sparse_user_item,\n",
    "                                 item_similarity,\n",
    "                                 user_id_to_idx,\n",
    "                                 article_id_to_idx,\n",
    "                                 idx_to_article_id,\n",
    "                                 item_popularity,\n",
    "                                 max_history_items=10):\n",
    "    \"\"\"\n",
    "    Generate prediction scores using optimized item-based collaborative filtering.\n",
    "    Uses only the most recent history items for efficiency.\n",
    "    \"\"\"\n",
    "    print(\"Generating predictions...\")\n",
    "    start_time = time.time()\n",
    "    predictions = []\n",
    "    \n",
    "    for idx, row in tqdm(test_behavior_data.iterrows(), total=len(test_behavior_data)):\n",
    "        impression_id = row.get('impression_id', idx)\n",
    "        user_id = row['user_id']\n",
    "        \n",
    "        if pd.isna(row['impressions']):\n",
    "            continue\n",
    "            \n",
    "        # Parse impression articles\n",
    "        impression_articles = []\n",
    "        for imp in row['impressions'].split():\n",
    "            parts = imp.split('-')\n",
    "            article_id = parts[0]\n",
    "            impression_articles.append(article_id)\n",
    "        \n",
    "        # Get user's history\n",
    "        history_indices = []\n",
    "        \n",
    "        if user_id in user_id_to_idx and pd.notna(row['history']) and row['history'].strip() != \"\":\n",
    "            user_idx = user_id_to_idx[user_id]\n",
    "            history = row['history'].split()\n",
    "            \n",
    "            # Limit to most recent items for efficiency\n",
    "            if len(history) > max_history_items:\n",
    "                history = history[-max_history_items:]\n",
    "            \n",
    "            # Convert history articles to indices\n",
    "            for article_id in history:\n",
    "                if article_id in article_id_to_idx:\n",
    "                    history_indices.append(article_id_to_idx[article_id])\n",
    "        \n",
    "        # Score the impression articles\n",
    "        for article_id in impression_articles:\n",
    "            # Default score\n",
    "            score = 0.5\n",
    "            \n",
    "            if article_id in article_id_to_idx:\n",
    "                article_idx = article_id_to_idx[article_id]\n",
    "                \n",
    "                if history_indices:\n",
    "                    # Calculate weighted similarity to history articles\n",
    "                    total_sim = 0\n",
    "                    \n",
    "                    # Apply recency weighting to history items\n",
    "                    history_weights = np.linspace(0.6, 1.0, len(history_indices))\n",
    "                    \n",
    "                    for i, hist_idx in enumerate(history_indices):\n",
    "                        # Get similarity to this history item\n",
    "                        sim = item_similarity[hist_idx, article_idx]\n",
    "                        if sim:\n",
    "                            # Apply recency weight\n",
    "                            weight = history_weights[i]\n",
    "                            total_sim += sim * weight\n",
    "                    \n",
    "                    # Normalize by number of history items\n",
    "                    if len(history_indices) > 0:\n",
    "                        score = total_sim / sum(history_weights)\n",
    "                else:\n",
    "                    # No history - use item popularity\n",
    "                    score = 0.4 + 0.2 * (item_popularity[article_idx] / max(0.001, np.max(item_popularity)))\n",
    "            \n",
    "            # Ensure score is in [0, 1] range\n",
    "            score = min(1.0, max(0.0, score))\n",
    "            \n",
    "            # Add to predictions\n",
    "            predictions.append({\n",
    "                'impression_id': impression_id,\n",
    "                'news_id': article_id,\n",
    "                'score': float(score)\n",
    "            })\n",
    "    \n",
    "    # Create DataFrame\n",
    "    predictions_df = pd.DataFrame(predictions)\n",
    "    \n",
    "    # Ensure correct datatypes for evaluation\n",
    "    try:\n",
    "        predictions_df['impression_id'] = predictions_df['impression_id'].astype('int64')\n",
    "    except:\n",
    "        predictions_df['impression_id'] = predictions_df['impression_id'].astype(str)\n",
    "        \n",
    "    predictions_df['news_id'] = predictions_df['news_id'].astype(str)\n",
    "    predictions_df['score'] = predictions_df['score'].astype(float)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    print(f\"Prediction generation completed in {end_time - start_time:.2f} seconds\")\n",
    "    \n",
    "    return predictions_df\n",
    "\n",
    "def optimized_item_cf_pipeline(behavior_data, test_behavior_data, \n",
    "                            max_articles_per_user=20, \n",
    "                            max_items=30000,\n",
    "                            max_history_items=10):\n",
    "    \"\"\"\n",
    "    Complete pipeline for optimized item-based collaborative filtering.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    behavior_data : DataFrame\n",
    "        Training data containing user_id and history columns\n",
    "    test_behavior_data : DataFrame\n",
    "        Test data for generating predictions\n",
    "    max_articles_per_user : int\n",
    "        Maximum number of articles to consider per user\n",
    "    max_items : int\n",
    "        Maximum number of items to include in the model\n",
    "    max_history_items : int\n",
    "        Maximum number of history items to use when generating predictions\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    predictions_df : DataFrame\n",
    "        DataFrame with impression_id, news_id, and score columns\n",
    "    \"\"\"\n",
    "    print(f\"Starting optimized item-based collaborative filtering pipeline...\")\n",
    "    overall_start = time.time()\n",
    "    \n",
    "    # Process interactions\n",
    "    interactions_df = process_interactions_efficiently(\n",
    "        behavior_data, \n",
    "        max_articles_per_user=max_articles_per_user\n",
    "    )\n",
    "    \n",
    "    # Create matrices\n",
    "    (sparse_user_item, item_similarity, \n",
    "     user_id_to_idx, article_id_to_idx, \n",
    "     idx_to_user_id, idx_to_article_id,\n",
    "     item_popularity) = create_optimized_item_similarity(\n",
    "        interactions_df, \n",
    "        max_items=max_items\n",
    "    )\n",
    "    \n",
    "    # Generate predictions\n",
    "    predictions_df = generate_optimized_predictions(\n",
    "        test_behavior_data,\n",
    "        sparse_user_item, \n",
    "        item_similarity,\n",
    "        user_id_to_idx, \n",
    "        article_id_to_idx,\n",
    "        idx_to_article_id,\n",
    "        item_popularity,\n",
    "        max_history_items=max_history_items\n",
    "    )\n",
    "    \n",
    "    overall_end = time.time()\n",
    "    print(f\"Pipeline completed in {overall_end - overall_start:.2f} seconds\")\n",
    "    print(f\"Generated {len(predictions_df)} predictions\")\n",
    "    \n",
    "    return predictions_df\n",
    "\n",
    "# Example usage:\n",
    "sample_size = 30000  # Adjust based on your needs\n",
    "test_sample = test_behavior_data.sample(n=sample_size, random_state=42)\n",
    "predictions_df = optimized_item_cf_pipeline(\n",
    "    behavior_data, \n",
    "    test_sample,\n",
    "    max_articles_per_user=100,  # Focus on most recent articles per user\n",
    "    max_items=180000,           # Focus on most popular articles\n",
    "    max_history_items=80       # Use only most recent history when predicting\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'auc': np.float64(0.5136020804347559),\n",
       " 'mrr': np.float64(0.24575208262118636),\n",
       " 'ndcg@5': np.float64(0.24858198129150502),\n",
       " 'ndcg@10': np.float64(0.3130225646546266)}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(eval)\n",
    "\n",
    "eval.evaluate_mind_predictions(predictions_df, test_behavior_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
