{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>category</th>\n",
       "      <th>subcategory</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>url</th>\n",
       "      <th>title_entities</th>\n",
       "      <th>abstract_entities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>N88753</td>\n",
       "      <td>lifestyle</td>\n",
       "      <td>lifestyleroyals</td>\n",
       "      <td>The Brands Queen Elizabeth, Prince Charles, an...</td>\n",
       "      <td>Shop the notebooks, jackets, and more that the...</td>\n",
       "      <td>https://assets.msn.com/labs/mind/AAGH0ET.html</td>\n",
       "      <td>[{\"Label\": \"Prince Philip, Duke of Edinburgh\",...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>N45436</td>\n",
       "      <td>news</td>\n",
       "      <td>newsscienceandtechnology</td>\n",
       "      <td>Walmart Slashes Prices on Last-Generation iPads</td>\n",
       "      <td>Apple's new iPad releases bring big deals on l...</td>\n",
       "      <td>https://assets.msn.com/labs/mind/AABmf2I.html</td>\n",
       "      <td>[{\"Label\": \"IPad\", \"Type\": \"J\", \"WikidataId\": ...</td>\n",
       "      <td>[{\"Label\": \"IPad\", \"Type\": \"J\", \"WikidataId\": ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>N23144</td>\n",
       "      <td>health</td>\n",
       "      <td>weightloss</td>\n",
       "      <td>50 Worst Habits For Belly Fat</td>\n",
       "      <td>These seemingly harmless habits are holding yo...</td>\n",
       "      <td>https://assets.msn.com/labs/mind/AAB19MK.html</td>\n",
       "      <td>[{\"Label\": \"Adipose tissue\", \"Type\": \"C\", \"Wik...</td>\n",
       "      <td>[{\"Label\": \"Adipose tissue\", \"Type\": \"C\", \"Wik...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>N86255</td>\n",
       "      <td>health</td>\n",
       "      <td>medical</td>\n",
       "      <td>Dispose of unwanted prescription drugs during ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://assets.msn.com/labs/mind/AAISxPN.html</td>\n",
       "      <td>[{\"Label\": \"Drug Enforcement Administration\", ...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>N93187</td>\n",
       "      <td>news</td>\n",
       "      <td>newsworld</td>\n",
       "      <td>The Cost of Trump's Aid Freeze in the Trenches...</td>\n",
       "      <td>Lt. Ivan Molchanets peeked over a parapet of s...</td>\n",
       "      <td>https://assets.msn.com/labs/mind/AAJgNsz.html</td>\n",
       "      <td>[]</td>\n",
       "      <td>[{\"Label\": \"Ukraine\", \"Type\": \"G\", \"WikidataId...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  article_id   category               subcategory  \\\n",
       "0     N88753  lifestyle           lifestyleroyals   \n",
       "1     N45436       news  newsscienceandtechnology   \n",
       "2     N23144     health                weightloss   \n",
       "3     N86255     health                   medical   \n",
       "4     N93187       news                 newsworld   \n",
       "\n",
       "                                               title  \\\n",
       "0  The Brands Queen Elizabeth, Prince Charles, an...   \n",
       "1    Walmart Slashes Prices on Last-Generation iPads   \n",
       "2                      50 Worst Habits For Belly Fat   \n",
       "3  Dispose of unwanted prescription drugs during ...   \n",
       "4  The Cost of Trump's Aid Freeze in the Trenches...   \n",
       "\n",
       "                                            abstract  \\\n",
       "0  Shop the notebooks, jackets, and more that the...   \n",
       "1  Apple's new iPad releases bring big deals on l...   \n",
       "2  These seemingly harmless habits are holding yo...   \n",
       "3                                                NaN   \n",
       "4  Lt. Ivan Molchanets peeked over a parapet of s...   \n",
       "\n",
       "                                             url  \\\n",
       "0  https://assets.msn.com/labs/mind/AAGH0ET.html   \n",
       "1  https://assets.msn.com/labs/mind/AABmf2I.html   \n",
       "2  https://assets.msn.com/labs/mind/AAB19MK.html   \n",
       "3  https://assets.msn.com/labs/mind/AAISxPN.html   \n",
       "4  https://assets.msn.com/labs/mind/AAJgNsz.html   \n",
       "\n",
       "                                      title_entities  \\\n",
       "0  [{\"Label\": \"Prince Philip, Duke of Edinburgh\",...   \n",
       "1  [{\"Label\": \"IPad\", \"Type\": \"J\", \"WikidataId\": ...   \n",
       "2  [{\"Label\": \"Adipose tissue\", \"Type\": \"C\", \"Wik...   \n",
       "3  [{\"Label\": \"Drug Enforcement Administration\", ...   \n",
       "4                                                 []   \n",
       "\n",
       "                                   abstract_entities  \n",
       "0                                                 []  \n",
       "1  [{\"Label\": \"IPad\", \"Type\": \"J\", \"WikidataId\": ...  \n",
       "2  [{\"Label\": \"Adipose tissue\", \"Type\": \"C\", \"Wik...  \n",
       "3                                                 []  \n",
       "4  [{\"Label\": \"Ukraine\", \"Type\": \"G\", \"WikidataId...  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_data = pd.read_csv(\"data/MINDlarge_train/news.tsv\", header=None, sep='\\t')\n",
    "news_data.columns = ['article_id', 'category', 'subcategory', 'title', 'abstract', 'url', 'title_entities', 'abstract_entities']\n",
    "\n",
    "news_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>impression_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>history</th>\n",
       "      <th>impressions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>U87243</td>\n",
       "      <td>11/10/2019 11:30:54 AM</td>\n",
       "      <td>N8668 N39081 N65259 N79529 N73408 N43615 N2937...</td>\n",
       "      <td>N78206-0 N26368-0 N7578-0 N58592-0 N19858-0 N5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>U598644</td>\n",
       "      <td>11/12/2019 1:45:29 PM</td>\n",
       "      <td>N56056 N8726 N70353 N67998 N83823 N111108 N107...</td>\n",
       "      <td>N47996-0 N82719-0 N117066-0 N8491-0 N123784-0 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>U532401</td>\n",
       "      <td>11/13/2019 11:23:03 AM</td>\n",
       "      <td>N128643 N87446 N122948 N9375 N82348 N129412 N5...</td>\n",
       "      <td>N103852-0 N53474-0 N127836-0 N47925-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>U593596</td>\n",
       "      <td>11/12/2019 12:24:09 PM</td>\n",
       "      <td>N31043 N39592 N4104 N8223 N114581 N92747 N1207...</td>\n",
       "      <td>N38902-0 N76434-0 N71593-0 N100073-0 N108736-0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>U239687</td>\n",
       "      <td>11/14/2019 8:03:01 PM</td>\n",
       "      <td>N65250 N122359 N71723 N53796 N41663 N41484 N11...</td>\n",
       "      <td>N76209-0 N48841-0 N67937-0 N62235-0 N6307-0 N3...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   impression_id  user_id               timestamp  \\\n",
       "0              1   U87243  11/10/2019 11:30:54 AM   \n",
       "1              2  U598644   11/12/2019 1:45:29 PM   \n",
       "2              3  U532401  11/13/2019 11:23:03 AM   \n",
       "3              4  U593596  11/12/2019 12:24:09 PM   \n",
       "4              5  U239687   11/14/2019 8:03:01 PM   \n",
       "\n",
       "                                             history  \\\n",
       "0  N8668 N39081 N65259 N79529 N73408 N43615 N2937...   \n",
       "1  N56056 N8726 N70353 N67998 N83823 N111108 N107...   \n",
       "2  N128643 N87446 N122948 N9375 N82348 N129412 N5...   \n",
       "3  N31043 N39592 N4104 N8223 N114581 N92747 N1207...   \n",
       "4  N65250 N122359 N71723 N53796 N41663 N41484 N11...   \n",
       "\n",
       "                                         impressions  \n",
       "0  N78206-0 N26368-0 N7578-0 N58592-0 N19858-0 N5...  \n",
       "1  N47996-0 N82719-0 N117066-0 N8491-0 N123784-0 ...  \n",
       "2              N103852-0 N53474-0 N127836-0 N47925-1  \n",
       "3  N38902-0 N76434-0 N71593-0 N100073-0 N108736-0...  \n",
       "4  N76209-0 N48841-0 N67937-0 N62235-0 N6307-0 N3...  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "behavior_data = pd.read_csv(\"data/MINDlarge_train/behaviors.tsv\", header=None, sep='\\t')\n",
    "behavior_data.columns = ['impression_id', 'user_id', 'timestamp', 'history', 'impressions']\n",
    "\n",
    "behavior_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we have to create the user-item matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm import tqdm  # Optional for progress tracking\n",
    "\n",
    "def process_interactions_efficiently(behavior_data, batch_size=10000):\n",
    "    \"\"\"\n",
    "    Process behavior data to extract user-item interactions efficiently using batching.\n",
    "    \"\"\"\n",
    "    total_batches = (len(behavior_data) + batch_size - 1) // batch_size\n",
    "    interaction_dfs = []\n",
    "    \n",
    "    for batch_num in range(total_batches):\n",
    "        # Get a batch of the data\n",
    "        start_idx = batch_num * batch_size\n",
    "        end_idx = min((batch_num + 1) * batch_size, len(behavior_data))\n",
    "        batch = behavior_data.iloc[start_idx:end_idx]\n",
    "        \n",
    "        # Filter rows with valid history\n",
    "        valid_rows = batch[batch['history'].notna() & (batch['history'].str.strip() != '')]\n",
    "        \n",
    "        if len(valid_rows) > 0:\n",
    "            # Apply vectorized operations\n",
    "            temp_df = valid_rows[['user_id', 'history']].copy()\n",
    "            temp_df['article_id'] = temp_df['history'].str.split()\n",
    "            # Explode to create one row per user-article interaction\n",
    "            temp_df = temp_df.explode('article_id')\n",
    "            # Keep only the columns we need\n",
    "            temp_df = temp_df[['user_id', 'article_id']]\n",
    "            \n",
    "            interaction_dfs.append(temp_df)\n",
    "    \n",
    "    # Combine all batches into final dataframe\n",
    "    if interaction_dfs:\n",
    "        interactions_df = pd.concat(interaction_dfs, ignore_index=True)\n",
    "    else:\n",
    "        interactions_df = pd.DataFrame(columns=['user_id', 'article_id'])\n",
    "        \n",
    "    return interactions_df\n",
    "\n",
    "def create_sparse_matrices(interactions_df):\n",
    "    \"\"\"\n",
    "    Create sparse user-item matrix and compute sparse item similarity matrix.\n",
    "    Returns both matrices and mapping dictionaries.\n",
    "    \"\"\"\n",
    "    # Create mappings from IDs to indices\n",
    "    user_ids = interactions_df['user_id'].unique()\n",
    "    article_ids = interactions_df['article_id'].unique()\n",
    "    \n",
    "    user_id_to_idx = {id: i for i, id in enumerate(user_ids)}\n",
    "    article_id_to_idx = {id: i for i, id in enumerate(article_ids)}\n",
    "    \n",
    "    # Map the original IDs to matrix indices\n",
    "    user_indices = interactions_df['user_id'].map(user_id_to_idx).values\n",
    "    article_indices = interactions_df['article_id'].map(article_id_to_idx).values\n",
    "    \n",
    "    # Create interaction values (all 1s for implicit feedback)\n",
    "    interaction_values = np.ones(len(interactions_df), dtype=np.float32)\n",
    "    \n",
    "    # Create the sparse user-item matrix\n",
    "    sparse_user_item = csr_matrix(\n",
    "        (interaction_values, (user_indices, article_indices)),\n",
    "        shape=(len(user_ids), len(article_ids))\n",
    "    )\n",
    "    \n",
    "    # Create item-item similarity matrix (cosine similarity between items)\n",
    "    print(\"Computing item similarity matrix (this might take a while)...\")\n",
    "    sparse_item_similarity = cosine_similarity(sparse_user_item.T, dense_output=False)\n",
    "    \n",
    "    # Create reverse mappings to convert back to original IDs\n",
    "    idx_to_user_id = {i: id for id, i in user_id_to_idx.items()}\n",
    "    idx_to_article_id = {i: id for id, i in article_id_to_idx.items()}\n",
    "    \n",
    "    return (sparse_user_item, sparse_item_similarity, \n",
    "            user_id_to_idx, article_id_to_idx, \n",
    "            idx_to_user_id, idx_to_article_id)\n",
    "\n",
    "def get_item_recommendations_sparse(user_id, \n",
    "                                  sparse_user_item, \n",
    "                                  sparse_item_similarity,\n",
    "                                  user_id_to_idx, \n",
    "                                  article_id_to_idx,\n",
    "                                  idx_to_article_id,\n",
    "                                  top_n=5):\n",
    "    \"\"\"\n",
    "    Generate top-n item recommendations for a given user using sparse matrices.\n",
    "    \"\"\"\n",
    "    # Convert user_id to matrix index\n",
    "    if user_id not in user_id_to_idx:\n",
    "        return []\n",
    "    \n",
    "    user_idx = user_id_to_idx[user_id]\n",
    "    \n",
    "    # Get items the user has interacted with\n",
    "    user_interactions = sparse_user_item[user_idx].toarray().flatten()\n",
    "    interacted_item_indices = np.where(user_interactions > 0)[0]\n",
    "    \n",
    "    if len(interacted_item_indices) == 0:\n",
    "        return []  # User has no interactions, cannot recommend\n",
    "    \n",
    "    # Initialize scores array for all items\n",
    "    scores = np.zeros(sparse_item_similarity.shape[0])\n",
    "    \n",
    "    # For each item the user has interacted with\n",
    "    for item_idx in interacted_item_indices:\n",
    "        # Get similarity scores for this item with all other items\n",
    "        similarity_scores = sparse_item_similarity[item_idx].toarray().flatten()\n",
    "        # Add to accumulated scores\n",
    "        scores += similarity_scores\n",
    "    \n",
    "    # Set scores of items the user has already interacted with to -1 (to exclude them)\n",
    "    scores[interacted_item_indices] = -1\n",
    "    \n",
    "    # Get indices of top_n items with highest scores\n",
    "    recommended_indices = np.argsort(scores)[::-1][:top_n]\n",
    "    \n",
    "    # Convert indices back to article IDs\n",
    "    recommended_articles = [idx_to_article_id[idx] for idx in recommended_indices if scores[idx] > 0]\n",
    "    \n",
    "    return recommended_articles\n",
    "\n",
    "def batch_generate_recommendations(user_ids, \n",
    "                                  sparse_user_item, \n",
    "                                  sparse_item_similarity,\n",
    "                                  user_id_to_idx, \n",
    "                                  article_id_to_idx,\n",
    "                                  idx_to_article_id,\n",
    "                                  top_n=5):\n",
    "    \"\"\"\n",
    "    Generate recommendations for multiple users efficiently.\n",
    "    \"\"\"\n",
    "    recommendations = {}\n",
    "    \n",
    "    for user_id in tqdm(user_ids, desc=\"Generating recommendations\"):\n",
    "        recs = get_item_recommendations_sparse(\n",
    "            user_id, \n",
    "            sparse_user_item, \n",
    "            sparse_item_similarity,\n",
    "            user_id_to_idx, \n",
    "            article_id_to_idx,\n",
    "            idx_to_article_id,\n",
    "            top_n=top_n\n",
    "        )\n",
    "        recommendations[user_id] = recs\n",
    "        \n",
    "    return recommendations\n",
    "\n",
    "\n",
    "def collaborative_filtering_pipeline(behavior_data, top_n=5, sample_users=None):\n",
    "    \"\"\"\n",
    "    Complete pipeline for collaborative filtering.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    behavior_data : DataFrame\n",
    "        DataFrame containing user_id and history columns\n",
    "    top_n : int\n",
    "        Number of recommendations per user\n",
    "    sample_users : list or None\n",
    "        List of specific user_ids to generate recommendations for,\n",
    "        or None to use all users\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple: (recommendations, sparse_user_item, sparse_item_similarity, \n",
    "            user_id_to_idx, article_id_to_idx, idx_to_user_id, idx_to_article_id)\n",
    "    \"\"\"\n",
    "    print(\"Processing interactions...\")\n",
    "    interactions_df = process_interactions_efficiently(behavior_data)\n",
    "    \n",
    "    print(\"Creating sparse matrices...\")\n",
    "    (sparse_user_item, sparse_item_similarity, \n",
    "     user_id_to_idx, article_id_to_idx, \n",
    "     idx_to_user_id, idx_to_article_id) = create_sparse_matrices(interactions_df)\n",
    "    \n",
    "    print(f\"User-item matrix shape: {sparse_user_item.shape}\")\n",
    "    print(f\"Density: {sparse_user_item.nnz / (sparse_user_item.shape[0] * sparse_user_item.shape[1]):.6f}\")\n",
    "    \n",
    "    if sample_users is None:\n",
    "        # Use all users (or first 100 for demonstration)\n",
    "        sample_users = list(user_id_to_idx.keys())[:500]  # Limit for demonstration\n",
    "    \n",
    "    print(f\"Generating recommendations for {len(sample_users)} users...\")\n",
    "    recommendations = batch_generate_recommendations(\n",
    "        sample_users,\n",
    "        sparse_user_item, \n",
    "        sparse_item_similarity,\n",
    "        user_id_to_idx, \n",
    "        article_id_to_idx,\n",
    "        idx_to_article_id,\n",
    "        top_n=top_n\n",
    "    )\n",
    "    \n",
    "    # Return all necessary variables for evaluation\n",
    "    return (recommendations, sparse_user_item, sparse_item_similarity, \n",
    "            user_id_to_idx, article_id_to_idx, idx_to_user_id, idx_to_article_id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>impression_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>history</th>\n",
       "      <th>impressions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>U134050</td>\n",
       "      <td>11/15/2019 8:55:22 AM</td>\n",
       "      <td>N12246 N128820 N119226 N4065 N67770 N33446 N10...</td>\n",
       "      <td>N91737-0 N30206-0 N54368-0 N117802-0 N18190-0 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>U254959</td>\n",
       "      <td>11/15/2019 11:42:35 AM</td>\n",
       "      <td>N34011 N9375 N67397 N7936 N118985 N109453 N103...</td>\n",
       "      <td>N119999-0 N24958-0 N104054-0 N33901-0 N9250-0 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>U499841</td>\n",
       "      <td>11/15/2019 9:08:21 AM</td>\n",
       "      <td>N63858 N26834 N6379 N85484 N15229 N65119 N1047...</td>\n",
       "      <td>N18190-0 N89764-0 N91737-0 N54368-0 N49978-1 N...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>U107107</td>\n",
       "      <td>11/15/2019 5:50:31 AM</td>\n",
       "      <td>N12959 N8085 N18389 N3758 N9740 N90543 N129790...</td>\n",
       "      <td>N122944-1 N18190-0 N55801-0 N59297-0 N128045-0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>U492344</td>\n",
       "      <td>11/15/2019 5:02:25 AM</td>\n",
       "      <td>N109183 N48453 N85005 N45706 N98923 N46069 N35...</td>\n",
       "      <td>N64785-0 N82503-0 N32993-0 N122944-0 N29160-0 ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   impression_id  user_id               timestamp  \\\n",
       "0              1  U134050   11/15/2019 8:55:22 AM   \n",
       "1              2  U254959  11/15/2019 11:42:35 AM   \n",
       "2              3  U499841   11/15/2019 9:08:21 AM   \n",
       "3              4  U107107   11/15/2019 5:50:31 AM   \n",
       "4              5  U492344   11/15/2019 5:02:25 AM   \n",
       "\n",
       "                                             history  \\\n",
       "0  N12246 N128820 N119226 N4065 N67770 N33446 N10...   \n",
       "1  N34011 N9375 N67397 N7936 N118985 N109453 N103...   \n",
       "2  N63858 N26834 N6379 N85484 N15229 N65119 N1047...   \n",
       "3  N12959 N8085 N18389 N3758 N9740 N90543 N129790...   \n",
       "4  N109183 N48453 N85005 N45706 N98923 N46069 N35...   \n",
       "\n",
       "                                         impressions  \n",
       "0  N91737-0 N30206-0 N54368-0 N117802-0 N18190-0 ...  \n",
       "1  N119999-0 N24958-0 N104054-0 N33901-0 N9250-0 ...  \n",
       "2  N18190-0 N89764-0 N91737-0 N54368-0 N49978-1 N...  \n",
       "3  N122944-1 N18190-0 N55801-0 N59297-0 N128045-0...  \n",
       "4  N64785-0 N82503-0 N32993-0 N122944-0 N29160-0 ...  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_behavior_data = pd.read_csv(\"data/MINDlarge_dev/behaviors.tsv\", header=None, sep='\\t')\n",
    "test_behavior_data.columns = ['impression_id', 'user_id', 'timestamp', 'history', 'impressions']\n",
    "\n",
    "test_behavior_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing interactions...\n",
      "Creating sparse matrices...\n",
      "Computing item similarity matrix (this might take a while)...\n",
      "User-item matrix shape: (698365, 79546)\n",
      "Density: 0.000237\n",
      "Generating recommendations for 500 users...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating recommendations: 100%|██████████| 500/500 [00:04<00:00, 106.56it/s]\n",
      "100%|██████████| 30000/30000 [03:27<00:00, 144.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 1128901 predictions\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>impression_id</th>\n",
       "      <th>news_id</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>181577</td>\n",
       "      <td>N83707</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>181577</td>\n",
       "      <td>N26122</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>181577</td>\n",
       "      <td>N32993</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>181577</td>\n",
       "      <td>N80770</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>181577</td>\n",
       "      <td>N86609</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  impression_id news_id  score\n",
       "0        181577  N83707    0.5\n",
       "1        181577  N26122    0.5\n",
       "2        181577  N32993    0.5\n",
       "3        181577  N80770    0.5\n",
       "4        181577  N86609    0.5"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "def generate_mind_predictions(test_behavior_data, \n",
    "                                         sparse_user_item, \n",
    "                                         sparse_item_similarity,\n",
    "                                         user_id_to_idx, \n",
    "                                         article_id_to_idx,\n",
    "                                         idx_to_article_id):\n",
    "    \"\"\"\n",
    "    Generate prediction scores with compatible datatypes for evaluation.\n",
    "    \"\"\"\n",
    "    predictions = []\n",
    "    \n",
    "    for idx, row in tqdm(test_behavior_data.iterrows(), total=len(test_behavior_data)):\n",
    "        # Get impression_id (use row index if not in columns)\n",
    "        impression_id = row.get('impression_id', idx)\n",
    "        user_id = row['user_id']\n",
    "        \n",
    "        if pd.isna(row['impressions']):\n",
    "            continue\n",
    "            \n",
    "        # Parse impression articles\n",
    "        for imp in row['impressions'].split():\n",
    "            parts = imp.split('-')\n",
    "            article_id = parts[0]\n",
    "            \n",
    "            # Calculate score (using default 0.5 for simplicity in this example)\n",
    "            score = 0.5\n",
    "            \n",
    "            # If user and article exist in our model, calculate real score\n",
    "            if user_id in user_id_to_idx and article_id in article_id_to_idx:\n",
    "                user_idx = user_id_to_idx[user_id]\n",
    "                article_idx = article_id_to_idx[article_id]\n",
    "                \n",
    "                # Get user's interaction history\n",
    "                user_interactions = sparse_user_item[user_idx].toarray().flatten()\n",
    "                interacted_item_indices = np.where(user_interactions > 0)[0]\n",
    "                \n",
    "                if len(interacted_item_indices) > 0:\n",
    "                    # Calculate similarity-based score\n",
    "                    total_sim = 0\n",
    "                    \n",
    "                    for item_idx in interacted_item_indices:\n",
    "                        # Get similarity safely\n",
    "                        sim_matrix = sparse_item_similarity[item_idx].toarray().flatten()\n",
    "                        if article_idx < len(sim_matrix):\n",
    "                            total_sim += sim_matrix[article_idx]\n",
    "                    \n",
    "                    # Normalize to 0-1 range (simple approach)\n",
    "                    # This is a simplified normalization - adjust if needed\n",
    "                    score = min(1.0, max(0.0, total_sim / max(1, len(interacted_item_indices))))\n",
    "            \n",
    "            # Append to predictions list\n",
    "            predictions.append({\n",
    "                'impression_id': impression_id,\n",
    "                'news_id': article_id,\n",
    "                'score': float(score)  # Ensure float\n",
    "            })\n",
    "    \n",
    "    # Create DataFrame with explicit dtypes\n",
    "    predictions_df = pd.DataFrame(predictions)\n",
    "    \n",
    "    # Ensure correct datatypes for Polars conversion\n",
    "    predictions_df['impression_id'] = predictions_df['impression_id'].astype(str)\n",
    "    predictions_df['news_id'] = predictions_df['news_id'].astype(str)\n",
    "    predictions_df['score'] = predictions_df['score'].astype(float)\n",
    "    \n",
    "    \n",
    "    \n",
    "    return predictions_df\n",
    "\n",
    "# Example usage:\n",
    "# This code should be placed after running the collaborative_filtering_pipeline\n",
    "\n",
    "(recommendations, sparse_user_item, sparse_item_similarity, \n",
    "            user_id_to_idx, article_id_to_idx, idx_to_user_id, idx_to_article_id) = collaborative_filtering_pipeline(behavior_data, top_n=5)\n",
    "\n",
    "sample_size = 30000  # Adjust this based on your needs\n",
    "test_sample = test_behavior_data.sample(n=sample_size, random_state=42)\n",
    "\n",
    "# Generate predictions for test data\n",
    "predictions_df = generate_mind_predictions(\n",
    "    test_sample,\n",
    "    sparse_user_item, \n",
    "    sparse_item_similarity,\n",
    "    user_id_to_idx, \n",
    "    article_id_to_idx,\n",
    "    idx_to_article_id\n",
    ")\n",
    "\n",
    "# You can then save predictions to a CSV file for submission or evaluation\n",
    "predictions_df.to_csv('mind_predictions.csv', index=False)\n",
    "\n",
    "print(f\"Generated {len(predictions_df)} predictions\")\n",
    "predictions_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_df['impression_id'] = predictions_df['impression_id'].astype('int64')  # i64\n",
    "predictions_df['news_id'] = predictions_df['news_id'].astype(str)                 # str\n",
    "predictions_df['score'] = predictions_df['score'].astype('float64')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>impression_id</th>\n",
       "      <th>news_id</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>181577</td>\n",
       "      <td>N83707</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>181577</td>\n",
       "      <td>N26122</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>181577</td>\n",
       "      <td>N32993</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>181577</td>\n",
       "      <td>N80770</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>181577</td>\n",
       "      <td>N86609</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   impression_id news_id  score\n",
       "0         181577  N83707    0.5\n",
       "1         181577  N26122    0.5\n",
       "2         181577  N32993    0.5\n",
       "3         181577  N80770    0.5\n",
       "4         181577  N86609    0.5"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'auc': np.float64(0.5122522263430606),\n",
       " 'mrr': np.float64(0.2454378407400277),\n",
       " 'ndcg@5': np.float64(0.24792856661957594),\n",
       " 'ndcg@10': np.float64(0.31242379386645075)}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import lib.eval as eval\n",
    "import importlib\n",
    "\n",
    "importlib.reload(eval)\n",
    "\n",
    "eval.evaluate_mind_predictions(predictions_df, test_behavior_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing interactions...\n",
      "Creating sparse matrices...\n",
      "Computing item similarity matrix (this might take a while)...\n",
      "User-item matrix shape: (698365, 79546)\n",
      "Density: 0.000237\n",
      "Generating predictions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating predictions: 100%|██████████| 30000/30000 [02:15<00:00, 222.03it/s]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix, diags\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm import tqdm\n",
    "\n",
    "def process_interactions_efficiently(behavior_data, batch_size=10000):\n",
    "    \"\"\"\n",
    "    Process behavior data to extract user-item interactions efficiently using batching.\n",
    "    Added weighting based on position in history (recency).\n",
    "    \"\"\"\n",
    "    total_batches = (len(behavior_data) + batch_size - 1) // batch_size\n",
    "    interaction_dfs = []\n",
    "    \n",
    "    for batch_num in range(total_batches):\n",
    "        # Get a batch of the data\n",
    "        start_idx = batch_num * batch_size\n",
    "        end_idx = min((batch_num + 1) * batch_size, len(behavior_data))\n",
    "        batch = behavior_data.iloc[start_idx:end_idx]\n",
    "        \n",
    "        # Filter rows with valid history\n",
    "        valid_rows = batch[batch['history'].notna() & (batch['history'].str.strip() != '')]\n",
    "        \n",
    "        if len(valid_rows) > 0:\n",
    "            # Apply vectorized operations\n",
    "            temp_df = valid_rows[['user_id', 'history']].copy()\n",
    "            \n",
    "            # Process each user's history to add position weights\n",
    "            weighted_interactions = []\n",
    "            \n",
    "            for _, row in temp_df.iterrows():\n",
    "                user_id = row['user_id']\n",
    "                articles = row['history'].split()\n",
    "                \n",
    "                # Calculate position-based weights (more recent = higher weight)\n",
    "                # This captures the temporal aspect of news consumption\n",
    "                if len(articles) > 0:\n",
    "                    # Exponential weighting for stronger recency bias\n",
    "                    weights = np.exp(np.linspace(0, 1, len(articles))) / np.exp(1)\n",
    "                    \n",
    "                    for i, article_id in enumerate(articles):\n",
    "                        weighted_interactions.append({\n",
    "                            'user_id': user_id,\n",
    "                            'article_id': article_id,\n",
    "                            'weight': weights[i]\n",
    "                        })\n",
    "            \n",
    "            if weighted_interactions:\n",
    "                interaction_df = pd.DataFrame(weighted_interactions)\n",
    "                interaction_dfs.append(interaction_df)\n",
    "    \n",
    "    # Combine all batches into final dataframe\n",
    "    if interaction_dfs:\n",
    "        interactions_df = pd.concat(interaction_dfs, ignore_index=True)\n",
    "        \n",
    "        # Aggregate weights for duplicate user-item pairs (user may have clicked same article multiple times)\n",
    "        interactions_df = interactions_df.groupby(['user_id', 'article_id'])['weight'].sum().reset_index()\n",
    "    else:\n",
    "        interactions_df = pd.DataFrame(columns=['user_id', 'article_id', 'weight'])\n",
    "        \n",
    "    return interactions_df\n",
    "\n",
    "def create_sparse_matrices(interactions_df):\n",
    "    \"\"\"\n",
    "    Create sparse user-item matrix and compute improved item similarity matrix.\n",
    "    Uses TF-IDF weighting and normalization to improve recommendation quality.\n",
    "    \"\"\"\n",
    "    # Create mappings from IDs to indices\n",
    "    user_ids = interactions_df['user_id'].unique()\n",
    "    article_ids = interactions_df['article_id'].unique()\n",
    "    \n",
    "    user_id_to_idx = {id: i for i, id in enumerate(user_ids)}\n",
    "    article_id_to_idx = {id: i for i, id in enumerate(article_ids)}\n",
    "    \n",
    "    # Map the original IDs to matrix indices\n",
    "    user_indices = interactions_df['user_id'].map(user_id_to_idx).values\n",
    "    article_indices = interactions_df['article_id'].map(article_id_to_idx).values\n",
    "    \n",
    "    # Use weights instead of just 1s for interaction values\n",
    "    interaction_values = interactions_df['weight'].values\n",
    "    \n",
    "    # Create the sparse user-item matrix\n",
    "    sparse_user_item = csr_matrix(\n",
    "        (interaction_values, (user_indices, article_indices)),\n",
    "        shape=(len(user_ids), len(article_ids))\n",
    "    )\n",
    "    \n",
    "    # Apply TF-IDF weighting to reduce popularity bias\n",
    "    # First, compute IDF (inverse document frequency)\n",
    "    item_frequencies = np.array((sparse_user_item > 0).sum(axis=0)).flatten()\n",
    "    idf = np.log(sparse_user_item.shape[0] / (item_frequencies + 1))\n",
    "    \n",
    "    # Create a diagonal matrix with IDF values\n",
    "    idf_diag = diags(idf, 0)\n",
    "    \n",
    "    # Apply IDF weighting to user-item matrix\n",
    "    weighted_user_item = sparse_user_item.dot(idf_diag)\n",
    "    \n",
    "    # Normalize user vectors to unit length for better cosine similarity calculation\n",
    "    user_norm = np.sqrt(weighted_user_item.power(2).sum(axis=1))\n",
    "    user_norm[user_norm == 0] = 1  # Avoid division by zero\n",
    "    \n",
    "    row_diag = diags(1 / user_norm.A.flatten(), 0)\n",
    "    normalized_user_item = row_diag.dot(weighted_user_item)\n",
    "    \n",
    "    # Compute item popularity\n",
    "    item_popularity = item_frequencies / item_frequencies.sum()\n",
    "    \n",
    "    # Create item-item similarity matrix (cosine similarity between items)\n",
    "    print(\"Computing item similarity matrix (this might take a while)...\")\n",
    "    sparse_item_similarity = cosine_similarity(normalized_user_item.T, dense_output=False)\n",
    "    \n",
    "    # Create reverse mappings to convert back to original IDs\n",
    "    idx_to_user_id = {i: id for id, i in user_id_to_idx.items()}\n",
    "    idx_to_article_id = {i: id for id, i in article_id_to_idx.items()}\n",
    "    \n",
    "    return (normalized_user_item, sparse_item_similarity, \n",
    "            user_id_to_idx, article_id_to_idx, \n",
    "            idx_to_user_id, idx_to_article_id,\n",
    "            item_popularity)\n",
    "\n",
    "def get_item_recommendations_sparse(user_id, \n",
    "                                  sparse_user_item, \n",
    "                                  sparse_item_similarity,\n",
    "                                  user_id_to_idx, \n",
    "                                  article_id_to_idx,\n",
    "                                  idx_to_article_id,\n",
    "                                  item_popularity,\n",
    "                                  top_n=5,\n",
    "                                  diversity_factor=0.2,\n",
    "                                  popularity_penalty=0.1):\n",
    "    \"\"\"\n",
    "    Generate top-n item recommendations for a given user using sparse matrices.\n",
    "    Incorporates diversity and reduces popularity bias.\n",
    "    \"\"\"\n",
    "    # Convert user_id to matrix index\n",
    "    if user_id not in user_id_to_idx:\n",
    "        return []\n",
    "    \n",
    "    user_idx = user_id_to_idx[user_id]\n",
    "    \n",
    "    # Get items the user has interacted with\n",
    "    user_vector = sparse_user_item[user_idx].toarray().flatten()\n",
    "    interacted_item_indices = np.where(user_vector > 0)[0]\n",
    "    \n",
    "    if len(interacted_item_indices) == 0:\n",
    "        return []  # User has no interactions, cannot recommend\n",
    "    \n",
    "    # Initialize scores array for all items\n",
    "    scores = np.zeros(sparse_item_similarity.shape[0])\n",
    "    \n",
    "    # Apply a damping function to reduce popularity bias\n",
    "    popularity_penalty_factor = np.power(item_popularity, popularity_penalty)\n",
    "    \n",
    "    # Keep track of item similarity to already selected items for diversity\n",
    "    selected_items_similarity = np.zeros(sparse_item_similarity.shape[0])\n",
    "    \n",
    "    # For each item the user has interacted with, weighted by interaction strength\n",
    "    for i, item_idx in enumerate(interacted_item_indices):\n",
    "        # Get similarity scores for this item with all other items\n",
    "        similarity_scores = sparse_item_similarity[item_idx].toarray().flatten()\n",
    "        \n",
    "        # Weight by the user's interaction strength with this item\n",
    "        interaction_weight = user_vector[item_idx]\n",
    "        weighted_scores = similarity_scores * interaction_weight\n",
    "        \n",
    "        # Add to accumulated scores\n",
    "        scores += weighted_scores\n",
    "    \n",
    "    # Set scores of items the user has already interacted with to -1 (to exclude them)\n",
    "    scores[interacted_item_indices] = -1\n",
    "    \n",
    "    # Apply penalty for very popular items\n",
    "    scores = scores * (1 - popularity_penalty_factor)\n",
    "    \n",
    "    # Get final scores\n",
    "    valid_items = np.where(scores > 0)[0]\n",
    "    \n",
    "    if len(valid_items) == 0:\n",
    "        return []\n",
    "    \n",
    "    # Get recommendations balancing relevance and diversity\n",
    "    recommended_articles = []\n",
    "    remaining_items = valid_items.copy()\n",
    "    \n",
    "    for _ in range(min(top_n, len(valid_items))):\n",
    "        if len(remaining_items) == 0:\n",
    "            break\n",
    "            \n",
    "        # Compute diversity score\n",
    "        diversity_scores = np.zeros(len(remaining_items))\n",
    "        \n",
    "        if len(recommended_articles) > 0:\n",
    "            for i, item_idx in enumerate(remaining_items):\n",
    "                # Average similarity to already selected items\n",
    "                sim_to_selected = np.mean([\n",
    "                    sparse_item_similarity[item_idx, rec_idx].toarray()[0, 0]\n",
    "                    for rec_idx in recommended_articles\n",
    "                ])\n",
    "                # Lower similarity means more diversity\n",
    "                diversity_scores[i] = 1 - sim_to_selected\n",
    "        \n",
    "        # Get scores for remaining items\n",
    "        item_scores = np.array([scores[idx] for idx in remaining_items])\n",
    "        \n",
    "        # Combine relevance and diversity\n",
    "        if len(recommended_articles) > 0:\n",
    "            combined_scores = (1 - diversity_factor) * item_scores + diversity_factor * diversity_scores\n",
    "        else:\n",
    "            combined_scores = item_scores\n",
    "        \n",
    "        # Select the best item\n",
    "        best_idx = np.argmax(combined_scores)\n",
    "        selected_item = remaining_items[best_idx]\n",
    "        \n",
    "        # Add to recommendations\n",
    "        recommended_articles.append(selected_item)\n",
    "        \n",
    "        # Remove from remaining items\n",
    "        remaining_items = np.delete(remaining_items, best_idx)\n",
    "    \n",
    "    # Convert indices back to article IDs\n",
    "    return [idx_to_article_id[idx] for idx in recommended_articles]\n",
    "\n",
    "def generate_mind_predictions(test_behavior_data, \n",
    "                             sparse_user_item, \n",
    "                             sparse_item_similarity,\n",
    "                             user_id_to_idx, \n",
    "                             article_id_to_idx,\n",
    "                             idx_to_article_id,\n",
    "                             item_popularity):\n",
    "    \"\"\"\n",
    "    Generate prediction scores for each impression-article pair in the test dataset.\n",
    "    Optimized for MIND dataset evaluation.\n",
    "    \"\"\"\n",
    "    predictions = []\n",
    "    \n",
    "    for idx, row in tqdm(test_behavior_data.iterrows(), \n",
    "                        total=len(test_behavior_data), \n",
    "                        desc=\"Generating predictions\"):\n",
    "        \n",
    "        impression_id = row['impression_id'] if 'impression_id' in row else idx\n",
    "        user_id = row['user_id']\n",
    "        \n",
    "        # Skip if no impressions\n",
    "        if pd.isna(row['impressions']):\n",
    "            continue\n",
    "            \n",
    "        # Parse impressions to get article IDs\n",
    "        impression_articles = []\n",
    "        for imp in row['impressions'].split():\n",
    "            parts = imp.split('-')\n",
    "            if len(parts) > 0:\n",
    "                article_id = parts[0]\n",
    "                impression_articles.append(article_id)\n",
    "        \n",
    "        # Skip if user not in the training data - use a default approach\n",
    "        if user_id not in user_id_to_idx:\n",
    "            # Use item popularity as a fallback strategy for cold-start users\n",
    "            for article_id in impression_articles:\n",
    "                if article_id in article_id_to_idx:\n",
    "                    article_idx = article_id_to_idx[article_id]\n",
    "                    pop_score = item_popularity[article_idx]\n",
    "                    # Scale popularity to a reasonable range\n",
    "                    score = min(1.0, pop_score * 100)\n",
    "                else:\n",
    "                    score = 0.5  # Default score\n",
    "                \n",
    "                predictions.append({\n",
    "                    'impression_id': impression_id,\n",
    "                    'news_id': article_id,\n",
    "                    'score': score\n",
    "                })\n",
    "            continue\n",
    "        \n",
    "        # Get user index\n",
    "        user_idx = user_id_to_idx[user_id]\n",
    "        \n",
    "        # Get items the user has interacted with\n",
    "        user_vector = sparse_user_item[user_idx].toarray().flatten()\n",
    "        interacted_item_indices = np.where(user_vector > 0)[0]\n",
    "        \n",
    "        # Skip if user has no interactions - use popularity\n",
    "        if len(interacted_item_indices) == 0:\n",
    "            for article_id in impression_articles:\n",
    "                if article_id in article_id_to_idx:\n",
    "                    article_idx = article_id_to_idx[article_id]\n",
    "                    score = item_popularity[article_idx] * 100\n",
    "                else:\n",
    "                    score = 0.5\n",
    "                \n",
    "                predictions.append({\n",
    "                    'impression_id': impression_id,\n",
    "                    'news_id': article_id,\n",
    "                    'score': score\n",
    "                })\n",
    "            continue\n",
    "        \n",
    "        # Initialize scores array for all items\n",
    "        all_scores = np.zeros(sparse_item_similarity.shape[0])\n",
    "        \n",
    "        # For each item the user has interacted with\n",
    "        for item_idx in interacted_item_indices:\n",
    "            # Weight by the user's interaction strength with this item\n",
    "            interaction_weight = user_vector[item_idx]\n",
    "            \n",
    "            # Get similarity scores for this item with all other items\n",
    "            similarity_scores = sparse_item_similarity[item_idx].toarray().flatten()\n",
    "            \n",
    "            # Apply weighted contribution\n",
    "            all_scores += similarity_scores * interaction_weight\n",
    "        \n",
    "        # Process each impression article\n",
    "        for article_id in impression_articles:\n",
    "            if article_id in article_id_to_idx:\n",
    "                article_idx = article_id_to_idx[article_id]\n",
    "                raw_score = all_scores[article_idx]\n",
    "                \n",
    "                # Apply logarithmic scaling to prevent extremely small/large values\n",
    "                if raw_score > 0:\n",
    "                    score = min(1.0, 0.5 + 0.5 * np.log1p(raw_score) / np.log(10))\n",
    "                else:\n",
    "                    # If no similarity, use item popularity as a fallback\n",
    "                    score = 0.5 * item_popularity[article_idx] * 100\n",
    "            else:\n",
    "                # Article not in training data\n",
    "                score = 0.5  # Default score\n",
    "            \n",
    "            predictions.append({\n",
    "                'impression_id': impression_id,\n",
    "                'news_id': article_id,\n",
    "                'score': score\n",
    "            })\n",
    "    \n",
    "    # Create DataFrame from predictions\n",
    "    predictions_df = pd.DataFrame(predictions)\n",
    "    \n",
    "    # Ensure correct datatypes\n",
    "    predictions_df['impression_id'] = predictions_df['impression_id'].astype('int64')\n",
    "    predictions_df['news_id'] = predictions_df['news_id'].apply(lambda x: f'\"{x}\"')\n",
    "    predictions_df['score'] = predictions_df['score'].astype('float64')\n",
    "    \n",
    "    return predictions_df\n",
    "\n",
    "def collaborative_filtering_pipeline(behavior_data, test_behavior_data, top_n=5):\n",
    "    \"\"\"\n",
    "    Complete pipeline for improved collaborative filtering.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    behavior_data : DataFrame\n",
    "        Training data containing user_id and history columns\n",
    "    test_behavior_data : DataFrame\n",
    "        Test data for generating predictions\n",
    "    top_n : int\n",
    "        Number of recommendations per user\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    predictions_df : DataFrame\n",
    "        DataFrame with impression_id, news_id, and score columns\n",
    "    \"\"\"\n",
    "    print(\"Processing interactions...\")\n",
    "    interactions_df = process_interactions_efficiently(behavior_data)\n",
    "    \n",
    "    print(\"Creating sparse matrices...\")\n",
    "    (sparse_user_item, sparse_item_similarity, \n",
    "     user_id_to_idx, article_id_to_idx, \n",
    "     idx_to_user_id, idx_to_article_id,\n",
    "     item_popularity) = create_sparse_matrices(interactions_df)\n",
    "    \n",
    "    print(f\"User-item matrix shape: {sparse_user_item.shape}\")\n",
    "    print(f\"Density: {sparse_user_item.nnz / (sparse_user_item.shape[0] * sparse_user_item.shape[1]):.6f}\")\n",
    "    \n",
    "    print(\"Generating predictions...\")\n",
    "    predictions_df = generate_mind_predictions(\n",
    "        test_behavior_data,\n",
    "        sparse_user_item, \n",
    "        sparse_item_similarity,\n",
    "        user_id_to_idx, \n",
    "        article_id_to_idx,\n",
    "        idx_to_article_id,\n",
    "        item_popularity\n",
    "    )\n",
    "    \n",
    "    return predictions_df\n",
    "\n",
    "sample_size = 30000  # Adjust this based on your needs\n",
    "test_sample = test_behavior_data.sample(n=sample_size, random_state=42)\n",
    "predictions_df = collaborative_filtering_pipeline(behavior_data, test_sample)\n",
    "predictions_df.to_csv('improved_cf_predictions.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_df = predictions_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'error': 'No matching impression_id and news_id between predictions and ground truth'}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import lib.eval as eval\n",
    "import importlib\n",
    "\n",
    "importlib.reload(eval)\n",
    "\n",
    "eval.evaluate_mind_predictions(predictions_df, test_behavior_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>impression_id</th>\n",
       "      <th>news_id</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1128896</th>\n",
       "      <td>349492</td>\n",
       "      <td>\"N72977\"</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1128897</th>\n",
       "      <td>349492</td>\n",
       "      <td>\"N54368\"</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1128898</th>\n",
       "      <td>349492</td>\n",
       "      <td>\"N46894\"</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1128899</th>\n",
       "      <td>349492</td>\n",
       "      <td>\"N29160\"</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1128900</th>\n",
       "      <td>349492</td>\n",
       "      <td>\"N122944\"</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         impression_id    news_id  score\n",
       "1128896         349492   \"N72977\"    0.5\n",
       "1128897         349492   \"N54368\"    0.5\n",
       "1128898         349492   \"N46894\"    0.5\n",
       "1128899         349492   \"N29160\"    0.5\n",
       "1128900         349492  \"N122944\"    0.5"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_df.tail()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
