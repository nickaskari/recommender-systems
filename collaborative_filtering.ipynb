{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>category</th>\n",
       "      <th>subcategory</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>url</th>\n",
       "      <th>title_entities</th>\n",
       "      <th>abstract_entities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>N88753</td>\n",
       "      <td>lifestyle</td>\n",
       "      <td>lifestyleroyals</td>\n",
       "      <td>The Brands Queen Elizabeth, Prince Charles, an...</td>\n",
       "      <td>Shop the notebooks, jackets, and more that the...</td>\n",
       "      <td>https://assets.msn.com/labs/mind/AAGH0ET.html</td>\n",
       "      <td>[{\"Label\": \"Prince Philip, Duke of Edinburgh\",...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>N45436</td>\n",
       "      <td>news</td>\n",
       "      <td>newsscienceandtechnology</td>\n",
       "      <td>Walmart Slashes Prices on Last-Generation iPads</td>\n",
       "      <td>Apple's new iPad releases bring big deals on l...</td>\n",
       "      <td>https://assets.msn.com/labs/mind/AABmf2I.html</td>\n",
       "      <td>[{\"Label\": \"IPad\", \"Type\": \"J\", \"WikidataId\": ...</td>\n",
       "      <td>[{\"Label\": \"IPad\", \"Type\": \"J\", \"WikidataId\": ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>N23144</td>\n",
       "      <td>health</td>\n",
       "      <td>weightloss</td>\n",
       "      <td>50 Worst Habits For Belly Fat</td>\n",
       "      <td>These seemingly harmless habits are holding yo...</td>\n",
       "      <td>https://assets.msn.com/labs/mind/AAB19MK.html</td>\n",
       "      <td>[{\"Label\": \"Adipose tissue\", \"Type\": \"C\", \"Wik...</td>\n",
       "      <td>[{\"Label\": \"Adipose tissue\", \"Type\": \"C\", \"Wik...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>N86255</td>\n",
       "      <td>health</td>\n",
       "      <td>medical</td>\n",
       "      <td>Dispose of unwanted prescription drugs during ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://assets.msn.com/labs/mind/AAISxPN.html</td>\n",
       "      <td>[{\"Label\": \"Drug Enforcement Administration\", ...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>N93187</td>\n",
       "      <td>news</td>\n",
       "      <td>newsworld</td>\n",
       "      <td>The Cost of Trump's Aid Freeze in the Trenches...</td>\n",
       "      <td>Lt. Ivan Molchanets peeked over a parapet of s...</td>\n",
       "      <td>https://assets.msn.com/labs/mind/AAJgNsz.html</td>\n",
       "      <td>[]</td>\n",
       "      <td>[{\"Label\": \"Ukraine\", \"Type\": \"G\", \"WikidataId...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  article_id   category               subcategory  \\\n",
       "0     N88753  lifestyle           lifestyleroyals   \n",
       "1     N45436       news  newsscienceandtechnology   \n",
       "2     N23144     health                weightloss   \n",
       "3     N86255     health                   medical   \n",
       "4     N93187       news                 newsworld   \n",
       "\n",
       "                                               title  \\\n",
       "0  The Brands Queen Elizabeth, Prince Charles, an...   \n",
       "1    Walmart Slashes Prices on Last-Generation iPads   \n",
       "2                      50 Worst Habits For Belly Fat   \n",
       "3  Dispose of unwanted prescription drugs during ...   \n",
       "4  The Cost of Trump's Aid Freeze in the Trenches...   \n",
       "\n",
       "                                            abstract  \\\n",
       "0  Shop the notebooks, jackets, and more that the...   \n",
       "1  Apple's new iPad releases bring big deals on l...   \n",
       "2  These seemingly harmless habits are holding yo...   \n",
       "3                                                NaN   \n",
       "4  Lt. Ivan Molchanets peeked over a parapet of s...   \n",
       "\n",
       "                                             url  \\\n",
       "0  https://assets.msn.com/labs/mind/AAGH0ET.html   \n",
       "1  https://assets.msn.com/labs/mind/AABmf2I.html   \n",
       "2  https://assets.msn.com/labs/mind/AAB19MK.html   \n",
       "3  https://assets.msn.com/labs/mind/AAISxPN.html   \n",
       "4  https://assets.msn.com/labs/mind/AAJgNsz.html   \n",
       "\n",
       "                                      title_entities  \\\n",
       "0  [{\"Label\": \"Prince Philip, Duke of Edinburgh\",...   \n",
       "1  [{\"Label\": \"IPad\", \"Type\": \"J\", \"WikidataId\": ...   \n",
       "2  [{\"Label\": \"Adipose tissue\", \"Type\": \"C\", \"Wik...   \n",
       "3  [{\"Label\": \"Drug Enforcement Administration\", ...   \n",
       "4                                                 []   \n",
       "\n",
       "                                   abstract_entities  \n",
       "0                                                 []  \n",
       "1  [{\"Label\": \"IPad\", \"Type\": \"J\", \"WikidataId\": ...  \n",
       "2  [{\"Label\": \"Adipose tissue\", \"Type\": \"C\", \"Wik...  \n",
       "3                                                 []  \n",
       "4  [{\"Label\": \"Ukraine\", \"Type\": \"G\", \"WikidataId...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_data = pd.read_csv(\"data/MINDlarge_train/news.tsv\", header=None, sep='\\t')\n",
    "news_data.columns = ['article_id', 'category', 'subcategory', 'title', 'abstract', 'url', 'title_entities', 'abstract_entities']\n",
    "\n",
    "news_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>impression_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>history</th>\n",
       "      <th>impressions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>U87243</td>\n",
       "      <td>11/10/2019 11:30:54 AM</td>\n",
       "      <td>N8668 N39081 N65259 N79529 N73408 N43615 N2937...</td>\n",
       "      <td>N78206-0 N26368-0 N7578-0 N58592-0 N19858-0 N5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>U598644</td>\n",
       "      <td>11/12/2019 1:45:29 PM</td>\n",
       "      <td>N56056 N8726 N70353 N67998 N83823 N111108 N107...</td>\n",
       "      <td>N47996-0 N82719-0 N117066-0 N8491-0 N123784-0 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>U532401</td>\n",
       "      <td>11/13/2019 11:23:03 AM</td>\n",
       "      <td>N128643 N87446 N122948 N9375 N82348 N129412 N5...</td>\n",
       "      <td>N103852-0 N53474-0 N127836-0 N47925-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>U593596</td>\n",
       "      <td>11/12/2019 12:24:09 PM</td>\n",
       "      <td>N31043 N39592 N4104 N8223 N114581 N92747 N1207...</td>\n",
       "      <td>N38902-0 N76434-0 N71593-0 N100073-0 N108736-0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>U239687</td>\n",
       "      <td>11/14/2019 8:03:01 PM</td>\n",
       "      <td>N65250 N122359 N71723 N53796 N41663 N41484 N11...</td>\n",
       "      <td>N76209-0 N48841-0 N67937-0 N62235-0 N6307-0 N3...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   impression_id  user_id               timestamp  \\\n",
       "0              1   U87243  11/10/2019 11:30:54 AM   \n",
       "1              2  U598644   11/12/2019 1:45:29 PM   \n",
       "2              3  U532401  11/13/2019 11:23:03 AM   \n",
       "3              4  U593596  11/12/2019 12:24:09 PM   \n",
       "4              5  U239687   11/14/2019 8:03:01 PM   \n",
       "\n",
       "                                             history  \\\n",
       "0  N8668 N39081 N65259 N79529 N73408 N43615 N2937...   \n",
       "1  N56056 N8726 N70353 N67998 N83823 N111108 N107...   \n",
       "2  N128643 N87446 N122948 N9375 N82348 N129412 N5...   \n",
       "3  N31043 N39592 N4104 N8223 N114581 N92747 N1207...   \n",
       "4  N65250 N122359 N71723 N53796 N41663 N41484 N11...   \n",
       "\n",
       "                                         impressions  \n",
       "0  N78206-0 N26368-0 N7578-0 N58592-0 N19858-0 N5...  \n",
       "1  N47996-0 N82719-0 N117066-0 N8491-0 N123784-0 ...  \n",
       "2              N103852-0 N53474-0 N127836-0 N47925-1  \n",
       "3  N38902-0 N76434-0 N71593-0 N100073-0 N108736-0...  \n",
       "4  N76209-0 N48841-0 N67937-0 N62235-0 N6307-0 N3...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "behavior_data = pd.read_csv(\"data/MINDlarge_train/behaviors.tsv\", header=None, sep='\\t')\n",
    "behavior_data.columns = ['impression_id', 'user_id', 'timestamp', 'history', 'impressions']\n",
    "\n",
    "behavior_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we have to create the user-item matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm import tqdm  # Optional for progress tracking\n",
    "\n",
    "def process_interactions_efficiently(behavior_data, batch_size=10000):\n",
    "    \"\"\"\n",
    "    Process behavior data to extract user-item interactions efficiently using batching.\n",
    "    \"\"\"\n",
    "    total_batches = (len(behavior_data) + batch_size - 1) // batch_size\n",
    "    interaction_dfs = []\n",
    "    \n",
    "    for batch_num in range(total_batches):\n",
    "        # Get a batch of the data\n",
    "        start_idx = batch_num * batch_size\n",
    "        end_idx = min((batch_num + 1) * batch_size, len(behavior_data))\n",
    "        batch = behavior_data.iloc[start_idx:end_idx]\n",
    "        \n",
    "        # Filter rows with valid history\n",
    "        valid_rows = batch[batch['history'].notna() & (batch['history'].str.strip() != '')]\n",
    "        \n",
    "        if len(valid_rows) > 0:\n",
    "            # Apply vectorized operations\n",
    "            temp_df = valid_rows[['user_id', 'history']].copy()\n",
    "            temp_df['article_id'] = temp_df['history'].str.split()\n",
    "            # Explode to create one row per user-article interaction\n",
    "            temp_df = temp_df.explode('article_id')\n",
    "            # Keep only the columns we need\n",
    "            temp_df = temp_df[['user_id', 'article_id']]\n",
    "            \n",
    "            interaction_dfs.append(temp_df)\n",
    "    \n",
    "    # Combine all batches into final dataframe\n",
    "    if interaction_dfs:\n",
    "        interactions_df = pd.concat(interaction_dfs, ignore_index=True)\n",
    "    else:\n",
    "        interactions_df = pd.DataFrame(columns=['user_id', 'article_id'])\n",
    "        \n",
    "    return interactions_df\n",
    "\n",
    "def create_sparse_matrices(interactions_df):\n",
    "    \"\"\"\n",
    "    Create sparse user-item matrix and compute sparse item similarity matrix.\n",
    "    Returns both matrices and mapping dictionaries.\n",
    "    \"\"\"\n",
    "    # Create mappings from IDs to indices\n",
    "    user_ids = interactions_df['user_id'].unique()\n",
    "    article_ids = interactions_df['article_id'].unique()\n",
    "    \n",
    "    user_id_to_idx = {id: i for i, id in enumerate(user_ids)}\n",
    "    article_id_to_idx = {id: i for i, id in enumerate(article_ids)}\n",
    "    \n",
    "    # Map the original IDs to matrix indices\n",
    "    user_indices = interactions_df['user_id'].map(user_id_to_idx).values\n",
    "    article_indices = interactions_df['article_id'].map(article_id_to_idx).values\n",
    "    \n",
    "    # Create interaction values (all 1s for implicit feedback)\n",
    "    interaction_values = np.ones(len(interactions_df), dtype=np.float32)\n",
    "    \n",
    "    # Create the sparse user-item matrix\n",
    "    sparse_user_item = csr_matrix(\n",
    "        (interaction_values, (user_indices, article_indices)),\n",
    "        shape=(len(user_ids), len(article_ids))\n",
    "    )\n",
    "    \n",
    "    # Create item-item similarity matrix (cosine similarity between items)\n",
    "    print(\"Computing item similarity matrix (this might take a while)...\")\n",
    "    sparse_item_similarity = cosine_similarity(sparse_user_item.T, dense_output=False)\n",
    "    \n",
    "    # Create reverse mappings to convert back to original IDs\n",
    "    idx_to_user_id = {i: id for id, i in user_id_to_idx.items()}\n",
    "    idx_to_article_id = {i: id for id, i in article_id_to_idx.items()}\n",
    "    \n",
    "    return (sparse_user_item, sparse_item_similarity, \n",
    "            user_id_to_idx, article_id_to_idx, \n",
    "            idx_to_user_id, idx_to_article_id)\n",
    "\n",
    "def get_item_recommendations_sparse(user_id, \n",
    "                                  sparse_user_item, \n",
    "                                  sparse_item_similarity,\n",
    "                                  user_id_to_idx, \n",
    "                                  article_id_to_idx,\n",
    "                                  idx_to_article_id,\n",
    "                                  top_n=5):\n",
    "    \"\"\"\n",
    "    Generate top-n item recommendations for a given user using sparse matrices.\n",
    "    \"\"\"\n",
    "    # Convert user_id to matrix index\n",
    "    if user_id not in user_id_to_idx:\n",
    "        return []\n",
    "    \n",
    "    user_idx = user_id_to_idx[user_id]\n",
    "    \n",
    "    # Get items the user has interacted with\n",
    "    user_interactions = sparse_user_item[user_idx].toarray().flatten()\n",
    "    interacted_item_indices = np.where(user_interactions > 0)[0]\n",
    "    \n",
    "    if len(interacted_item_indices) == 0:\n",
    "        return []  # User has no interactions, cannot recommend\n",
    "    \n",
    "    # Initialize scores array for all items\n",
    "    scores = np.zeros(sparse_item_similarity.shape[0])\n",
    "    \n",
    "    # For each item the user has interacted with\n",
    "    for item_idx in interacted_item_indices:\n",
    "        # Get similarity scores for this item with all other items\n",
    "        similarity_scores = sparse_item_similarity[item_idx].toarray().flatten()\n",
    "        # Add to accumulated scores\n",
    "        scores += similarity_scores\n",
    "    \n",
    "    # Set scores of items the user has already interacted with to -1 (to exclude them)\n",
    "    scores[interacted_item_indices] = -1\n",
    "    \n",
    "    # Get indices of top_n items with highest scores\n",
    "    recommended_indices = np.argsort(scores)[::-1][:top_n]\n",
    "    \n",
    "    # Convert indices back to article IDs\n",
    "    recommended_articles = [idx_to_article_id[idx] for idx in recommended_indices if scores[idx] > 0]\n",
    "    \n",
    "    return recommended_articles\n",
    "\n",
    "def batch_generate_recommendations(user_ids, \n",
    "                                  sparse_user_item, \n",
    "                                  sparse_item_similarity,\n",
    "                                  user_id_to_idx, \n",
    "                                  article_id_to_idx,\n",
    "                                  idx_to_article_id,\n",
    "                                  top_n=5):\n",
    "    \"\"\"\n",
    "    Generate recommendations for multiple users efficiently.\n",
    "    \"\"\"\n",
    "    recommendations = {}\n",
    "    \n",
    "    for user_id in tqdm(user_ids, desc=\"Generating recommendations\"):\n",
    "        recs = get_item_recommendations_sparse(\n",
    "            user_id, \n",
    "            sparse_user_item, \n",
    "            sparse_item_similarity,\n",
    "            user_id_to_idx, \n",
    "            article_id_to_idx,\n",
    "            idx_to_article_id,\n",
    "            top_n=top_n\n",
    "        )\n",
    "        recommendations[user_id] = recs\n",
    "        \n",
    "    return recommendations\n",
    "\n",
    "\n",
    "def collaborative_filtering_pipeline(behavior_data, top_n=5, sample_users=None):\n",
    "    \"\"\"\n",
    "    Complete pipeline for collaborative filtering.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    behavior_data : DataFrame\n",
    "        DataFrame containing user_id and history columns\n",
    "    top_n : int\n",
    "        Number of recommendations per user\n",
    "    sample_users : list or None\n",
    "        List of specific user_ids to generate recommendations for,\n",
    "        or None to use all users\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple: (recommendations, sparse_user_item, sparse_item_similarity, \n",
    "            user_id_to_idx, article_id_to_idx, idx_to_user_id, idx_to_article_id)\n",
    "    \"\"\"\n",
    "    print(\"Processing interactions...\")\n",
    "    interactions_df = process_interactions_efficiently(behavior_data)\n",
    "    \n",
    "    print(\"Creating sparse matrices...\")\n",
    "    (sparse_user_item, sparse_item_similarity, \n",
    "     user_id_to_idx, article_id_to_idx, \n",
    "     idx_to_user_id, idx_to_article_id) = create_sparse_matrices(interactions_df)\n",
    "    \n",
    "    print(f\"User-item matrix shape: {sparse_user_item.shape}\")\n",
    "    print(f\"Density: {sparse_user_item.nnz / (sparse_user_item.shape[0] * sparse_user_item.shape[1]):.6f}\")\n",
    "    \n",
    "    if sample_users is None:\n",
    "        # Use all users (or first 100 for demonstration)\n",
    "        sample_users = list(user_id_to_idx.keys())[:100]  # Limit for demonstration\n",
    "    \n",
    "    print(f\"Generating recommendations for {len(sample_users)} users...\")\n",
    "    recommendations = batch_generate_recommendations(\n",
    "        sample_users,\n",
    "        sparse_user_item, \n",
    "        sparse_item_similarity,\n",
    "        user_id_to_idx, \n",
    "        article_id_to_idx,\n",
    "        idx_to_article_id,\n",
    "        top_n=top_n\n",
    "    )\n",
    "    \n",
    "    # Return all necessary variables for evaluation\n",
    "    return (recommendations, sparse_user_item, sparse_item_similarity, \n",
    "            user_id_to_idx, article_id_to_idx, idx_to_user_id, idx_to_article_id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>impression_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>history</th>\n",
       "      <th>impressions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>U134050</td>\n",
       "      <td>11/15/2019 8:55:22 AM</td>\n",
       "      <td>N12246 N128820 N119226 N4065 N67770 N33446 N10...</td>\n",
       "      <td>N91737-0 N30206-0 N54368-0 N117802-0 N18190-0 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>U254959</td>\n",
       "      <td>11/15/2019 11:42:35 AM</td>\n",
       "      <td>N34011 N9375 N67397 N7936 N118985 N109453 N103...</td>\n",
       "      <td>N119999-0 N24958-0 N104054-0 N33901-0 N9250-0 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>U499841</td>\n",
       "      <td>11/15/2019 9:08:21 AM</td>\n",
       "      <td>N63858 N26834 N6379 N85484 N15229 N65119 N1047...</td>\n",
       "      <td>N18190-0 N89764-0 N91737-0 N54368-0 N49978-1 N...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>U107107</td>\n",
       "      <td>11/15/2019 5:50:31 AM</td>\n",
       "      <td>N12959 N8085 N18389 N3758 N9740 N90543 N129790...</td>\n",
       "      <td>N122944-1 N18190-0 N55801-0 N59297-0 N128045-0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>U492344</td>\n",
       "      <td>11/15/2019 5:02:25 AM</td>\n",
       "      <td>N109183 N48453 N85005 N45706 N98923 N46069 N35...</td>\n",
       "      <td>N64785-0 N82503-0 N32993-0 N122944-0 N29160-0 ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   impression_id  user_id               timestamp  \\\n",
       "0              1  U134050   11/15/2019 8:55:22 AM   \n",
       "1              2  U254959  11/15/2019 11:42:35 AM   \n",
       "2              3  U499841   11/15/2019 9:08:21 AM   \n",
       "3              4  U107107   11/15/2019 5:50:31 AM   \n",
       "4              5  U492344   11/15/2019 5:02:25 AM   \n",
       "\n",
       "                                             history  \\\n",
       "0  N12246 N128820 N119226 N4065 N67770 N33446 N10...   \n",
       "1  N34011 N9375 N67397 N7936 N118985 N109453 N103...   \n",
       "2  N63858 N26834 N6379 N85484 N15229 N65119 N1047...   \n",
       "3  N12959 N8085 N18389 N3758 N9740 N90543 N129790...   \n",
       "4  N109183 N48453 N85005 N45706 N98923 N46069 N35...   \n",
       "\n",
       "                                         impressions  \n",
       "0  N91737-0 N30206-0 N54368-0 N117802-0 N18190-0 ...  \n",
       "1  N119999-0 N24958-0 N104054-0 N33901-0 N9250-0 ...  \n",
       "2  N18190-0 N89764-0 N91737-0 N54368-0 N49978-1 N...  \n",
       "3  N122944-1 N18190-0 N55801-0 N59297-0 N128045-0...  \n",
       "4  N64785-0 N82503-0 N32993-0 N122944-0 N29160-0 ...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_behavior_data = pd.read_csv(\"data/MINDlarge_dev/behaviors.tsv\", header=None, sep='\\t')\n",
    "test_behavior_data.columns = ['impression_id', 'user_id', 'timestamp', 'history', 'impressions']\n",
    "\n",
    "test_behavior_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing interactions...\n",
      "Creating sparse matrices...\n",
      "Computing item similarity matrix (this might take a while)...\n",
      "User-item matrix shape: (698365, 79546)\n",
      "Density: 0.000237\n",
      "Generating recommendations for 100 users...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating recommendations: 100%|██████████| 100/100 [00:00<00:00, 111.92it/s]\n",
      "100%|██████████| 10000/10000 [00:18<00:00, 544.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total predictions: 375460\n",
      "Matches found: 298856 (79.6%)\n",
      "Default scores used: 76604 (20.4%)\n",
      "Generated 375460 predictions\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>impression_id</th>\n",
       "      <th>news_id</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>181577</td>\n",
       "      <td>N83707</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>181577</td>\n",
       "      <td>N26122</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>181577</td>\n",
       "      <td>N32993</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>181577</td>\n",
       "      <td>N80770</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>181577</td>\n",
       "      <td>N86609</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   impression_id news_id  score\n",
       "0         181577  N83707    0.5\n",
       "1         181577  N26122    0.5\n",
       "2         181577  N32993    0.5\n",
       "3         181577  N80770    0.5\n",
       "4         181577  N86609    0.5"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "def generate_mind_predictions(test_behavior_data, \n",
    "                              sparse_user_item, \n",
    "                              sparse_item_similarity,\n",
    "                              user_id_to_idx, \n",
    "                              article_id_to_idx,\n",
    "                              idx_to_article_id,\n",
    "                              default_score=0.5):\n",
    "    \"\"\"\n",
    "    Generate prediction scores with improved debugging and robustness.\n",
    "    \"\"\"\n",
    "    predictions = []\n",
    "    matches_found = 0\n",
    "    default_used = 0\n",
    "    \n",
    "    for idx, row in tqdm(test_behavior_data.iterrows(), total=len(test_behavior_data)):\n",
    "        impression_id = row['impression_id'] if 'impression_id' in row else idx\n",
    "        user_id = row['user_id']\n",
    "        \n",
    "        if pd.isna(row['impressions']):\n",
    "            continue\n",
    "            \n",
    "        # Parse impression articles\n",
    "        impression_articles = []\n",
    "        for imp in row['impressions'].split():\n",
    "            parts = imp.split('-')\n",
    "            article_id = parts[0]\n",
    "            impression_articles.append(article_id)\n",
    "        \n",
    "        # Check if this user exists in training data\n",
    "        if user_id not in user_id_to_idx:\n",
    "            # Default scores for unknown user\n",
    "            for article_id in impression_articles:\n",
    "                predictions.append({\n",
    "                    'impression_id': impression_id,\n",
    "                    'news_id': article_id,\n",
    "                    'score': default_score\n",
    "                })\n",
    "            default_used += len(impression_articles)\n",
    "            continue\n",
    "        \n",
    "        # Get user interactions\n",
    "        user_idx = user_id_to_idx[user_id]\n",
    "        user_interactions = sparse_user_item[user_idx].toarray().flatten()\n",
    "        interacted_item_indices = np.where(user_interactions > 0)[0]\n",
    "        \n",
    "        if len(interacted_item_indices) == 0:\n",
    "            # Default scores for users with no interactions\n",
    "            for article_id in impression_articles:\n",
    "                predictions.append({\n",
    "                    'impression_id': impression_id,\n",
    "                    'news_id': article_id,\n",
    "                    'score': default_score\n",
    "                })\n",
    "            default_used += len(impression_articles)\n",
    "            continue\n",
    "        \n",
    "        # Calculate raw similarity scores\n",
    "        raw_scores = {}\n",
    "        valid_scores_found = False\n",
    "        \n",
    "        for article_id in impression_articles:\n",
    "            if article_id in article_id_to_idx:\n",
    "                article_idx = article_id_to_idx[article_id]\n",
    "                score = 0\n",
    "                \n",
    "                # Sum similarities from user's history\n",
    "                for item_idx in interacted_item_indices:\n",
    "                    sim = sparse_item_similarity[item_idx, article_idx]\n",
    "                    score += sim if sim else 0\n",
    "                \n",
    "                raw_scores[article_id] = score\n",
    "                if score > 0:\n",
    "                    valid_scores_found = True\n",
    "            else:\n",
    "                raw_scores[article_id] = 0\n",
    "        \n",
    "        # Normalize scores only if we found valid similarities\n",
    "        if valid_scores_found:\n",
    "            # Find min and max for normalization\n",
    "            scores_list = list(raw_scores.values())\n",
    "            min_score = min(scores_list)\n",
    "            max_score = max(scores_list)\n",
    "            \n",
    "            if max_score > min_score:\n",
    "                # Normalize\n",
    "                for article_id in impression_articles:\n",
    "                    norm_score = (raw_scores[article_id] - min_score) / (max_score - min_score)\n",
    "                    predictions.append({\n",
    "                        'impression_id': impression_id,\n",
    "                        'news_id': article_id,\n",
    "                        'score': norm_score\n",
    "                    })\n",
    "                matches_found += len(impression_articles)\n",
    "            else:\n",
    "                # All scores identical - use defaults\n",
    "                for article_id in impression_articles:\n",
    "                    predictions.append({\n",
    "                        'impression_id': impression_id,\n",
    "                        'news_id': article_id,\n",
    "                        'score': default_score\n",
    "                    })\n",
    "                default_used += len(impression_articles)\n",
    "        else:\n",
    "            # No valid scores - use defaults\n",
    "            for article_id in impression_articles:\n",
    "                predictions.append({\n",
    "                    'impression_id': impression_id,\n",
    "                    'news_id': article_id,\n",
    "                    'score': default_score\n",
    "                })\n",
    "            default_used += len(impression_articles)\n",
    "    \n",
    "    predictions_df = pd.DataFrame(predictions)\n",
    "    \n",
    "    # Print stats\n",
    "    print(f\"Total predictions: {len(predictions_df)}\")\n",
    "    print(f\"Matches found: {matches_found} ({matches_found/len(predictions_df)*100:.1f}%)\")\n",
    "    print(f\"Default scores used: {default_used} ({default_used/len(predictions_df)*100:.1f}%)\")\n",
    "    \n",
    "    return predictions_df\n",
    "\n",
    "# Example usage:\n",
    "# This code should be placed after running the collaborative_filtering_pipeline\n",
    "\n",
    "(recommendations, sparse_user_item, sparse_item_similarity, \n",
    "            user_id_to_idx, article_id_to_idx, idx_to_user_id, idx_to_article_id) = collaborative_filtering_pipeline(behavior_data, top_n=5)\n",
    "\n",
    "sample_size = 10000  # Adjust this based on your needs\n",
    "test_sample = test_behavior_data.sample(n=sample_size, random_state=42)\n",
    "\n",
    "# Generate predictions for test data\n",
    "predictions_df = generate_mind_predictions(\n",
    "    test_sample,\n",
    "    sparse_user_item, \n",
    "    sparse_item_similarity,\n",
    "    user_id_to_idx, \n",
    "    article_id_to_idx,\n",
    "    idx_to_article_id\n",
    ")\n",
    "\n",
    "# You can then save predictions to a CSV file for submission or evaluation\n",
    "predictions_df.to_csv('mind_predictions.csv', index=False)\n",
    "\n",
    "print(f\"Generated {len(predictions_df)} predictions\")\n",
    "predictions_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
