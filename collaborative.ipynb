{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'lib.models.content_based_2' from '/Users/mathiasraa/Desktop/ntnu/recommender-systems/lib/models/content_based_2.py'>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import lib.models.content_based_2 as content_based\n",
    "import lib.eval as eval\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import importlib\n",
    "importlib.reload(eval)\n",
    "importlib.reload(content_based)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "behavior_polars_train = pl.read_csv(\"data/MINDlarge_train/behaviors.tsv\", separator='\\t', has_header=False)\n",
    "behavior_polars_dev = pl.read_csv(\"data/MINDlarge_dev/behaviors.tsv\", separator='\\t', has_header=False)\n",
    "behavior_polars_train.columns = ['impression_id', 'user_id', 'time', 'history', 'impressions']\n",
    "behavior_polars_dev.columns = ['impression_id', 'user_id', 'time', 'history', 'impressions']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_train = pl.read_csv(\"data/MINDlarge_train/news.tsv\", separator='\\t', has_header=False, quote_char=None)\n",
    "news_dev = pl.read_csv(\"data/MINDlarge_dev/news.tsv\", separator='\\t', has_header=False, quote_char=None)\n",
    "news_train.columns = ['news_id', 'category', 'subcategory', 'title', 'abstract', 'url', 'title_entities', 'abstract_entities']\n",
    "news_dev.columns = ['news_id', 'category', 'subcategory', 'title', 'abstract', 'url', 'title_entities', 'abstract_entities']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_behaviors(behaviors_df):\n",
    "    \"\"\"\n",
    "    Fast processing of behaviors data using Polars' vectorized operations.\n",
    "    \n",
    "    Args:\n",
    "        behaviors_df: DataFrame with impression logs (pandas or polars)\n",
    "        \n",
    "    Returns:\n",
    "        Polars DataFrame with user-news interactions\n",
    "    \"\"\"\n",
    "    # Convert to polars if needed\n",
    "    if not isinstance(behaviors_df, pl.DataFrame):\n",
    "        behaviors_pl = pl.from_pandas(behaviors_df)\n",
    "    else:\n",
    "        behaviors_pl = behaviors_df\n",
    "    \n",
    "    # Convert time to datetime\n",
    "    behaviors_pl = behaviors_pl.with_columns(\n",
    "        pl.col(\"time\").str.to_datetime(\"%m/%d/%Y %I:%M:%S %p\")\n",
    "    )\n",
    "    \n",
    "    # Filter rows with valid impressions\n",
    "    behaviors_pl = behaviors_pl.filter(\n",
    "        ~pl.col(\"impressions\").is_null() & (pl.col(\"impressions\") != \"\")\n",
    "    )\n",
    "    \n",
    "    # Split the impressions string into a list column\n",
    "    with_splits = behaviors_pl.with_columns(\n",
    "        pl.col(\"impressions\").str.split(by=\" \").alias(\"impression_list\")\n",
    "    )\n",
    "    \n",
    "    # Explode the list column\n",
    "    exploded = with_splits.explode(\"impression_list\")\n",
    "    \n",
    "    # Extract news_id and click from impression string\n",
    "    processed = exploded.with_columns([\n",
    "        pl.col(\"impression_list\").str.split(\"-\").list.get(0).alias(\"news_id\"),\n",
    "        pl.col(\"impression_list\").str.split(\"-\").list.get(1).cast(pl.Int32, strict=False).alias(\"click\"),\n",
    "    ])\n",
    "    \n",
    "    # Select only valid entries and necessary columns\n",
    "    result = processed.filter(\n",
    "        ~pl.col(\"news_id\").is_null() & ~pl.col(\"click\").is_null()\n",
    "    ).select([\n",
    "        \"user_id\", \"impression_id\", \"time\", \"news_id\", \"click\"\n",
    "    ])\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "def create_interaction_matrix(interactions_df):\n",
    "    \"\"\"\n",
    "    Create a user-item interaction matrix.\n",
    "    \n",
    "    Args:\n",
    "        interactions_df: DataFrame with user_id, news_id, and click\n",
    "        \n",
    "    Returns:\n",
    "        User-item matrix, user_map, item_map\n",
    "    \"\"\"\n",
    "    # Filter to only clicked items (click=1)\n",
    "    clicked_interactions = interactions_df[interactions_df['click'] == 1]\n",
    "    \n",
    "    # Create mappings for users and items to matrix indices\n",
    "    user_ids = clicked_interactions['user_id'].unique()\n",
    "    news_ids = clicked_interactions['news_id'].unique()\n",
    "    \n",
    "    user_map = {user_id: idx for idx, user_id in enumerate(user_ids)}\n",
    "    news_map = {news_id: idx for idx, news_id in enumerate(news_ids)}\n",
    "    \n",
    "    # Create the interaction matrix\n",
    "    rows = clicked_interactions['user_id'].map(user_map).values\n",
    "    cols = clicked_interactions['news_id'].map(news_map).values\n",
    "    values = np.ones(len(rows))  # Binary interaction\n",
    "    \n",
    "    interaction_matrix = csr_matrix((values, (rows, cols)), \n",
    "                                   shape=(len(user_ids), len(news_ids)))\n",
    "    \n",
    "    return interaction_matrix, user_map, news_map\n",
    "\n",
    "def compute_item_similarity(interaction_matrix):\n",
    "    \"\"\"\n",
    "    Compute item-item similarity using cosine similarity.\n",
    "    \n",
    "    Args:\n",
    "        interaction_matrix: User-item interaction matrix\n",
    "        \n",
    "    Returns:\n",
    "        Item-item similarity matrix\n",
    "    \"\"\"\n",
    "    # Transpose to get item features (users who interacted with each item)\n",
    "    item_features = interaction_matrix.T\n",
    "    \n",
    "    # Compute cosine similarity between items\n",
    "    item_similarity = cosine_similarity(item_features)\n",
    "    \n",
    "    return item_similarity\n",
    "\n",
    "def compute_user_similarity(interaction_matrix):\n",
    "    \"\"\"\n",
    "    Compute user-user similarity using cosine similarity.\n",
    "    \n",
    "    Args:\n",
    "        interaction_matrix: User-item interaction matrix\n",
    "        \n",
    "    Returns:\n",
    "        User-user similarity matrix\n",
    "    \"\"\"\n",
    "    # Compute cosine similarity between users\n",
    "    user_similarity = cosine_similarity(interaction_matrix)\n",
    "    \n",
    "    return user_similarity\n",
    "\n",
    "def get_user_history(user_id, behaviors_df):\n",
    "    \"\"\"\n",
    "    Get a user's news click history.\n",
    "    \n",
    "    Args:\n",
    "        user_id: ID of the user\n",
    "        behaviors_df: DataFrame with behavior data\n",
    "        \n",
    "    Returns:\n",
    "        List of news IDs the user has clicked\n",
    "    \"\"\"\n",
    "    # Get all history entries for the user\n",
    "    user_behaviors = behaviors_df[behaviors_df['user_id'] == user_id]\n",
    "    \n",
    "    # Extract clicked news IDs from history\n",
    "    clicked_news = set()\n",
    "    for history in user_behaviors['history']:\n",
    "        if isinstance(history, str) and history.strip():\n",
    "            clicked_news.update(history.split())\n",
    "    \n",
    "    return list(clicked_news)\n",
    "\n",
    "def generate_news_recommendations(user_id, interaction_matrix, item_similarity, \n",
    "                                user_map, news_map, n_recommendations=10):\n",
    "    \"\"\"\n",
    "    Generate item-based collaborative filtering recommendations for news.\n",
    "    \n",
    "    Args:\n",
    "        user_id: ID of the user to generate recommendations for\n",
    "        interaction_matrix: User-item interaction matrix\n",
    "        item_similarity: Item-item similarity matrix\n",
    "        user_map: Mapping from user IDs to matrix indices\n",
    "        news_map: Mapping from news IDs to matrix indices\n",
    "        n_recommendations: Number of recommendations to generate\n",
    "        \n",
    "    Returns:\n",
    "        List of recommended news IDs\n",
    "    \"\"\"\n",
    "    # Check if user is in the training set\n",
    "    if user_id not in user_map:\n",
    "        return []\n",
    "    \n",
    "    # Map user_id to matrix index\n",
    "    user_idx = user_map[user_id]\n",
    "    \n",
    "    # Get items the user has already interacted with\n",
    "    user_interactions = interaction_matrix[user_idx].toarray().flatten()\n",
    "    already_interacted = np.where(user_interactions > 0)[0]\n",
    "    \n",
    "    # If user hasn't interacted with any items, return empty list\n",
    "    if len(already_interacted) == 0:\n",
    "        return []\n",
    "    \n",
    "    # Reverse the news mapping to get original IDs\n",
    "    reverse_news_map = {idx: news_id for news_id, idx in news_map.items()}\n",
    "    \n",
    "    # Calculate scores for each news item\n",
    "    scores = np.zeros(interaction_matrix.shape[1])\n",
    "    \n",
    "    for item_idx in already_interacted:\n",
    "        # Get similarity of this news to all other news\n",
    "        similarity_scores = item_similarity[item_idx]\n",
    "        # Add weighted similarities to scores\n",
    "        scores += similarity_scores * user_interactions[item_idx]\n",
    "    \n",
    "    # Set scores of already interacted items to -1 to exclude them\n",
    "    scores[already_interacted] = -1\n",
    "    \n",
    "    # Get top N recommendations\n",
    "    top_items_idx = np.argsort(scores)[::-1][:n_recommendations]\n",
    "    recommendations = [reverse_news_map[idx] for idx in top_items_idx]\n",
    "    \n",
    "    return recommendations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "\n",
    "\n",
    "def create_hybrid_similarity_matrix(interaction_matrix, news_df, news_map):\n",
    "    \"\"\"\n",
    "    Create a hybrid similarity matrix incorporating both collaborative signals\n",
    "    and content-based similarity.\n",
    "    \"\"\"\n",
    "    # Create content features from news categories and subcategories\n",
    "    news_ids = list(news_map.keys())\n",
    "    n_news = len(news_ids)\n",
    "    \n",
    "    # Create one-hot encoding for categories and subcategories\n",
    "    categories = news_df['category'].unique().tolist()\n",
    "    subcategories = news_df['subcategory'].unique().tolist()\n",
    "    \n",
    "    cat_map = {cat: i for i, cat in enumerate(categories)}\n",
    "    subcat_map = {subcat: i for i, subcat in enumerate(subcategories)}\n",
    "    \n",
    "    # Initialize feature matrix\n",
    "    # Features: [category_oh, subcategory_oh]\n",
    "    feat_matrix = np.zeros((n_news, len(categories) + len(subcategories)))\n",
    "    \n",
    "    for news_id, idx in news_map.items():\n",
    "        news_info = news_df[news_df['news_id'] == news_id]\n",
    "        if len(news_info) > 0:\n",
    "            # Add category one-hot\n",
    "            cat = news_info.iloc[0]['category']\n",
    "            if cat in cat_map:\n",
    "                feat_matrix[idx, cat_map[cat]] = 1.0\n",
    "                \n",
    "            # Add subcategory one-hot\n",
    "            subcat = news_info.iloc[0]['subcategory']\n",
    "            if subcat in subcat_map:\n",
    "                feat_matrix[idx, len(categories) + subcat_map[subcat]] = 1.0\n",
    "    \n",
    "    # Compute content-based similarity\n",
    "    content_similarity = cosine_similarity(feat_matrix)\n",
    "    \n",
    "    # Compute collaborative similarity\n",
    "    collaborative_similarity = cosine_similarity(interaction_matrix.T)\n",
    "    \n",
    "    # Create hybrid similarity (0.7 * collaborative + 0.3 * content)\n",
    "    # Adjust these weights based on your dataset\n",
    "    hybrid_similarity = 0.7 * collaborative_similarity + 0.3 * content_similarity\n",
    "    \n",
    "    return hybrid_similarity\n",
    "\n",
    "def sample_and_evaluate(test_interactions, interaction_matrix, similarity_matrix, \n",
    "                       user_map, news_map, news_df, behaviors_df, sample_size=1000):\n",
    "    \"\"\"\n",
    "    Sample users and evaluate the model\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Sample users who exist in both test and training\n",
    "    test_users = set(test_interactions[\"user_id\"].unique().to_list())\n",
    "    train_users = set(user_map.keys())\n",
    "    valid_users = list(test_users.intersection(train_users))\n",
    "    \n",
    "    sample_size = min(sample_size, len(valid_users))\n",
    "    sampled_users = random.sample(valid_users, sample_size)\n",
    "    \n",
    "    # Filter test data to only include sampled users\n",
    "    test_sample = test_interactions.filter(pl.col(\"user_id\").is_in(sampled_users))\n",
    "\n",
    "    test_actual_sample = behavior_polars_dev.filter(pl.col(\"user_id\").is_in(sampled_users))\n",
    "    \n",
    "    # Extract unique impression IDs in the sample\n",
    "    impression_ids = test_sample[\"impression_id\"].unique()\n",
    "    \n",
    "    # Create a map to look up user_id by impression_id \n",
    "    impression_to_user = {row[\"impression_id\"]: row[\"user_id\"] \n",
    "                          for row in test_sample.select([\"impression_id\", \"user_id\"]).unique().iter_rows(named=True)}\n",
    "    \n",
    "    # Extract all news IDs that need to be scored\n",
    "    news_to_score = test_sample.select([\"impression_id\", \"news_id\"]).unique()\n",
    "    \n",
    "    # Generate predictions\n",
    "    predictions = []\n",
    "    for row in news_to_score.iter_rows(named=True):\n",
    "        impression_id = row[\"impression_id\"]\n",
    "        news_id = row[\"news_id\"]\n",
    "        \n",
    "        # Skip if impression or news not in maps\n",
    "        if impression_id not in impression_to_user or news_id not in news_map:\n",
    "            continue\n",
    "        \n",
    "        user_id = impression_to_user[impression_id]\n",
    "        if user_id not in user_map:\n",
    "            continue\n",
    "            \n",
    "        user_idx = user_map[user_id]\n",
    "        news_idx = news_map[news_id]\n",
    "        \n",
    "        # Get user interactions\n",
    "        user_interactions = interaction_matrix[user_idx].toarray().flatten()\n",
    "        interacted_indices = np.where(user_interactions > 0)[0]\n",
    "        \n",
    "        if len(interacted_indices) == 0:\n",
    "            continue\n",
    "            \n",
    "        # Calculate score by combining collaborative and recency factors\n",
    "        collab_score = 0.0\n",
    "        for idx in interacted_indices:\n",
    "            collab_score += similarity_matrix[idx, news_idx] * user_interactions[idx]\n",
    "        \n",
    "        # Add time decay for older news (if applicable)\n",
    "        # This would require time information for each news article\n",
    "        \n",
    "        # Add prediction\n",
    "        predictions.append({\n",
    "            \"impression_id\": impression_id,\n",
    "            \"news_id\": news_id,\n",
    "            \"score\": float(collab_score)\n",
    "        })\n",
    "    \n",
    "    # Create prediction DataFrame\n",
    "    if predictions:\n",
    "        predictions_df = pl.DataFrame(predictions)\n",
    "        print(f\"Generated {len(predictions_df)} predictions in {time.time() - start_time:.2f} seconds\")\n",
    "        \n",
    "        # Evaluate predictions\n",
    "        eval_results = eval.evaluate_mind_predictions(\n",
    "            predictions_df,\n",
    "            behaviors_df=test_actual_sample,\n",
    "            metrics=[\"auc\", \"mrr\", \"ndcg@5\", \"ndcg@10\"]\n",
    "        )\n",
    "        \n",
    "        return eval_results\n",
    "    else:\n",
    "        print(\"No predictions generated\")\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df = news_train.to_pandas()\n",
    "behaviors_df = behavior_polars_train.to_pandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactions_df = preprocess_behaviors(behavior_polars_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_interactions = preprocess_behaviors(behavior_polars_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_interactions, test_interactions = interactions_df, test_interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Create interaction matrix from training data\n",
    "interaction_matrix, user_map, news_map = create_interaction_matrix(train_interactions.to_pandas())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Compute news-news similarity\n",
    "item_similarity = compute_item_similarity(interaction_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 216778 users that exist in both training and test data\n",
      "Sampled 3000 users for evaluation\n",
      "Sample contains 167173 interactions\n",
      "Unique pairs to score: 167173\n",
      "Generating predictions...\n",
      "Processing batch 1/168\n",
      "Processing batch 11/168\n",
      "Processing batch 21/168\n",
      "Processing batch 31/168\n",
      "Processing batch 41/168\n",
      "Processing batch 51/168\n",
      "Processing batch 61/168\n",
      "Processing batch 71/168\n",
      "Processing batch 81/168\n",
      "Processing batch 91/168\n",
      "Processing batch 101/168\n",
      "Processing batch 111/168\n",
      "Processing batch 121/168\n",
      "Processing batch 131/168\n",
      "Processing batch 141/168\n",
      "Processing batch 151/168\n",
      "Processing batch 161/168\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import polars as pl\n",
    "import random\n",
    "import time\n",
    "\n",
    "# Start timing\n",
    "start_time = time.time()\n",
    "\n",
    "# Sample 1000 users who exist in both test and training data\n",
    "test_users = set(test_interactions[\"user_id\"].unique().to_list())\n",
    "train_users = set(user_map.keys())\n",
    "valid_users = list(test_users.intersection(train_users))\n",
    "\n",
    "print(f\"Found {len(valid_users)} users that exist in both training and test data\")\n",
    "\n",
    "# Sample 1000 users or fewer if there aren't that many\n",
    "sample_size = min(3000, len(valid_users))\n",
    "sampled_users = random.sample(valid_users, sample_size)\n",
    "print(f\"Sampled {len(sampled_users)} users for evaluation\")\n",
    "\n",
    "# Filter test data to only include sampled users\n",
    "test_sample = test_interactions.filter(pl.col(\"user_id\").is_in(sampled_users))\n",
    "print(f\"Sample contains {len(test_sample)} interactions\")\n",
    "\n",
    "test_actual_sample = behavior_polars_dev.filter(pl.col(\"user_id\").is_in(sampled_users))\n",
    "\n",
    "# Extract unique user-news pairs to score\n",
    "unique_pairs = test_sample.select([\"impression_id\", \"user_id\", \"news_id\"]).unique()\n",
    "print(f\"Unique pairs to score: {len(unique_pairs)}\")\n",
    "\n",
    "# Convert to pandas for easier processing\n",
    "pairs_pd = unique_pairs.to_pandas()\n",
    "\n",
    "# Initialize predictions list\n",
    "predictions = []\n",
    "\n",
    "# Process in batches for better progress tracking\n",
    "batch_size = 1000\n",
    "num_batches = (len(pairs_pd) + batch_size - 1) // batch_size\n",
    "\n",
    "print(\"Generating predictions...\")\n",
    "for batch_idx in range(num_batches):\n",
    "    start_idx = batch_idx * batch_size\n",
    "    end_idx = min((batch_idx + 1) * batch_size, len(pairs_pd))\n",
    "    \n",
    "    if batch_idx % 10 == 0:\n",
    "        print(f\"Processing batch {batch_idx+1}/{num_batches}\")\n",
    "    \n",
    "    batch = pairs_pd.iloc[start_idx:end_idx]\n",
    "    \n",
    "    for _, row in batch.iterrows():\n",
    "        user_id = row['user_id']\n",
    "        news_id = row['news_id']\n",
    "        impression_id = row['impression_id']\n",
    "        \n",
    "        # Skip if news not in training\n",
    "        if news_id not in news_map:\n",
    "            continue\n",
    "            \n",
    "        # Get user profile\n",
    "        user_idx = user_map[user_id]\n",
    "        news_idx = news_map[news_id]\n",
    "        \n",
    "        # Get user interactions\n",
    "        user_interactions = interaction_matrix[user_idx].toarray().flatten()\n",
    "        interacted_indices = np.where(user_interactions > 0)[0]\n",
    "        \n",
    "        if len(interacted_indices) == 0:\n",
    "            continue\n",
    "            \n",
    "        # Calculate score\n",
    "        score = 0.0\n",
    "        for idx in interacted_indices:\n",
    "            if idx < item_similarity.shape[0] and news_idx < item_similarity.shape[1]:\n",
    "                score += item_similarity[idx, news_idx] * user_interactions[idx]\n",
    "        \n",
    "        # Add prediction\n",
    "        predictions.append({\n",
    "            \"impression_id\": impression_id,\n",
    "            \"news_id\": news_id,\n",
    "            \"score\": float(score)\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>auc</th>\n",
       "      <th>mrr</th>\n",
       "      <th>ndcg@5</th>\n",
       "      <th>ndcg@10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.509739</td>\n",
       "      <td>0.232029</td>\n",
       "      <td>0.240954</td>\n",
       "      <td>0.28879</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        auc       mrr    ndcg@5  ndcg@10\n",
       "0  0.509739  0.232029  0.240954  0.28879"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_df = pd.DataFrame(predictions)\n",
    "\n",
    "eval_results = eval.evaluate_mind_predictions(\n",
    "    predictions_df,\n",
    "    behaviors_df=test_actual_sample,  # Only use the sampled test data\n",
    "    metrics=[\"auc\", \"mrr\", \"ndcg@5\", \"ndcg@10\"]\n",
    ")\n",
    "\n",
    "pd.DataFrame(eval_results, index=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "hybrid_similarity = create_hybrid_similarity_matrix(interaction_matrix, news_df, news_map)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 39100 predictions in 2.28 seconds\n"
     ]
    }
   ],
   "source": [
    "# Evaluate with the enhanced approach\n",
    "results = sample_and_evaluate(\n",
    "    test_interactions, \n",
    "    interaction_matrix, \n",
    "    hybrid_similarity,\n",
    "    user_map, \n",
    "    news_map, \n",
    "    news_df,\n",
    "    behaviors_df,\n",
    "    sample_size=1000\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>auc</th>\n",
       "      <th>mrr</th>\n",
       "      <th>ndcg@5</th>\n",
       "      <th>ndcg@10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.581852</td>\n",
       "      <td>0.281212</td>\n",
       "      <td>0.293306</td>\n",
       "      <td>0.345894</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        auc       mrr    ndcg@5   ndcg@10\n",
       "0  0.581852  0.281212  0.293306  0.345894"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(results, index=[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
