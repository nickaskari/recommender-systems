{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm import tqdm\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>category</th>\n",
       "      <th>subcategory</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>url</th>\n",
       "      <th>title_entities</th>\n",
       "      <th>abstract_entities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>N88753</td>\n",
       "      <td>lifestyle</td>\n",
       "      <td>lifestyleroyals</td>\n",
       "      <td>The Brands Queen Elizabeth, Prince Charles, an...</td>\n",
       "      <td>Shop the notebooks, jackets, and more that the...</td>\n",
       "      <td>https://assets.msn.com/labs/mind/AAGH0ET.html</td>\n",
       "      <td>[{\"Label\": \"Prince Philip, Duke of Edinburgh\",...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>N45436</td>\n",
       "      <td>news</td>\n",
       "      <td>newsscienceandtechnology</td>\n",
       "      <td>Walmart Slashes Prices on Last-Generation iPads</td>\n",
       "      <td>Apple's new iPad releases bring big deals on l...</td>\n",
       "      <td>https://assets.msn.com/labs/mind/AABmf2I.html</td>\n",
       "      <td>[{\"Label\": \"IPad\", \"Type\": \"J\", \"WikidataId\": ...</td>\n",
       "      <td>[{\"Label\": \"IPad\", \"Type\": \"J\", \"WikidataId\": ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>N23144</td>\n",
       "      <td>health</td>\n",
       "      <td>weightloss</td>\n",
       "      <td>50 Worst Habits For Belly Fat</td>\n",
       "      <td>These seemingly harmless habits are holding yo...</td>\n",
       "      <td>https://assets.msn.com/labs/mind/AAB19MK.html</td>\n",
       "      <td>[{\"Label\": \"Adipose tissue\", \"Type\": \"C\", \"Wik...</td>\n",
       "      <td>[{\"Label\": \"Adipose tissue\", \"Type\": \"C\", \"Wik...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>N86255</td>\n",
       "      <td>health</td>\n",
       "      <td>medical</td>\n",
       "      <td>Dispose of unwanted prescription drugs during ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://assets.msn.com/labs/mind/AAISxPN.html</td>\n",
       "      <td>[{\"Label\": \"Drug Enforcement Administration\", ...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>N93187</td>\n",
       "      <td>news</td>\n",
       "      <td>newsworld</td>\n",
       "      <td>The Cost of Trump's Aid Freeze in the Trenches...</td>\n",
       "      <td>Lt. Ivan Molchanets peeked over a parapet of s...</td>\n",
       "      <td>https://assets.msn.com/labs/mind/AAJgNsz.html</td>\n",
       "      <td>[]</td>\n",
       "      <td>[{\"Label\": \"Ukraine\", \"Type\": \"G\", \"WikidataId...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  article_id   category               subcategory  \\\n",
       "0     N88753  lifestyle           lifestyleroyals   \n",
       "1     N45436       news  newsscienceandtechnology   \n",
       "2     N23144     health                weightloss   \n",
       "3     N86255     health                   medical   \n",
       "4     N93187       news                 newsworld   \n",
       "\n",
       "                                               title  \\\n",
       "0  The Brands Queen Elizabeth, Prince Charles, an...   \n",
       "1    Walmart Slashes Prices on Last-Generation iPads   \n",
       "2                      50 Worst Habits For Belly Fat   \n",
       "3  Dispose of unwanted prescription drugs during ...   \n",
       "4  The Cost of Trump's Aid Freeze in the Trenches...   \n",
       "\n",
       "                                            abstract  \\\n",
       "0  Shop the notebooks, jackets, and more that the...   \n",
       "1  Apple's new iPad releases bring big deals on l...   \n",
       "2  These seemingly harmless habits are holding yo...   \n",
       "3                                                NaN   \n",
       "4  Lt. Ivan Molchanets peeked over a parapet of s...   \n",
       "\n",
       "                                             url  \\\n",
       "0  https://assets.msn.com/labs/mind/AAGH0ET.html   \n",
       "1  https://assets.msn.com/labs/mind/AABmf2I.html   \n",
       "2  https://assets.msn.com/labs/mind/AAB19MK.html   \n",
       "3  https://assets.msn.com/labs/mind/AAISxPN.html   \n",
       "4  https://assets.msn.com/labs/mind/AAJgNsz.html   \n",
       "\n",
       "                                      title_entities  \\\n",
       "0  [{\"Label\": \"Prince Philip, Duke of Edinburgh\",...   \n",
       "1  [{\"Label\": \"IPad\", \"Type\": \"J\", \"WikidataId\": ...   \n",
       "2  [{\"Label\": \"Adipose tissue\", \"Type\": \"C\", \"Wik...   \n",
       "3  [{\"Label\": \"Drug Enforcement Administration\", ...   \n",
       "4                                                 []   \n",
       "\n",
       "                                   abstract_entities  \n",
       "0                                                 []  \n",
       "1  [{\"Label\": \"IPad\", \"Type\": \"J\", \"WikidataId\": ...  \n",
       "2  [{\"Label\": \"Adipose tissue\", \"Type\": \"C\", \"Wik...  \n",
       "3                                                 []  \n",
       "4  [{\"Label\": \"Ukraine\", \"Type\": \"G\", \"WikidataId...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_data = pd.read_csv(\"data/MINDlarge_train/news.tsv\", header=None, sep='\\t')\n",
    "news_data.columns = ['article_id', 'category', 'subcategory', 'title', 'abstract', 'url', 'title_entities', 'abstract_entities']\n",
    "\n",
    "news_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>impression_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>history</th>\n",
       "      <th>impressions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>U87243</td>\n",
       "      <td>11/10/2019 11:30:54 AM</td>\n",
       "      <td>N8668 N39081 N65259 N79529 N73408 N43615 N2937...</td>\n",
       "      <td>N78206-0 N26368-0 N7578-0 N58592-0 N19858-0 N5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>U598644</td>\n",
       "      <td>11/12/2019 1:45:29 PM</td>\n",
       "      <td>N56056 N8726 N70353 N67998 N83823 N111108 N107...</td>\n",
       "      <td>N47996-0 N82719-0 N117066-0 N8491-0 N123784-0 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>U532401</td>\n",
       "      <td>11/13/2019 11:23:03 AM</td>\n",
       "      <td>N128643 N87446 N122948 N9375 N82348 N129412 N5...</td>\n",
       "      <td>N103852-0 N53474-0 N127836-0 N47925-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>U593596</td>\n",
       "      <td>11/12/2019 12:24:09 PM</td>\n",
       "      <td>N31043 N39592 N4104 N8223 N114581 N92747 N1207...</td>\n",
       "      <td>N38902-0 N76434-0 N71593-0 N100073-0 N108736-0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>U239687</td>\n",
       "      <td>11/14/2019 8:03:01 PM</td>\n",
       "      <td>N65250 N122359 N71723 N53796 N41663 N41484 N11...</td>\n",
       "      <td>N76209-0 N48841-0 N67937-0 N62235-0 N6307-0 N3...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   impression_id  user_id               timestamp  \\\n",
       "0              1   U87243  11/10/2019 11:30:54 AM   \n",
       "1              2  U598644   11/12/2019 1:45:29 PM   \n",
       "2              3  U532401  11/13/2019 11:23:03 AM   \n",
       "3              4  U593596  11/12/2019 12:24:09 PM   \n",
       "4              5  U239687   11/14/2019 8:03:01 PM   \n",
       "\n",
       "                                             history  \\\n",
       "0  N8668 N39081 N65259 N79529 N73408 N43615 N2937...   \n",
       "1  N56056 N8726 N70353 N67998 N83823 N111108 N107...   \n",
       "2  N128643 N87446 N122948 N9375 N82348 N129412 N5...   \n",
       "3  N31043 N39592 N4104 N8223 N114581 N92747 N1207...   \n",
       "4  N65250 N122359 N71723 N53796 N41663 N41484 N11...   \n",
       "\n",
       "                                         impressions  \n",
       "0  N78206-0 N26368-0 N7578-0 N58592-0 N19858-0 N5...  \n",
       "1  N47996-0 N82719-0 N117066-0 N8491-0 N123784-0 ...  \n",
       "2              N103852-0 N53474-0 N127836-0 N47925-1  \n",
       "3  N38902-0 N76434-0 N71593-0 N100073-0 N108736-0...  \n",
       "4  N76209-0 N48841-0 N67937-0 N62235-0 N6307-0 N3...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "behavior_data = pd.read_csv(\"data/MINDlarge_train/behaviors.tsv\", header=None, sep='\\t')\n",
    "behavior_data.columns = ['impression_id', 'user_id', 'timestamp', 'history', 'impressions']\n",
    "\n",
    "behavior_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "test_behavior_data = pd.read_csv(\"data/MINDlarge_dev/behaviors.tsv\", header=None, sep='\\t')\n",
    "test_behavior_data.columns = ['impression_id', 'user_id', 'timestamp', 'history', 'impressions']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting MIND-optimized recommender...\n",
      "Processing interactions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting interactions:  99%|█████████▉| 2205937/2232748 [01:21<00:01, 23123.02it/s]"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "def process_interactions(behavior_data, use_timestamps=True):\n",
    "    \"\"\"\n",
    "    Process behavior data to extract user-item interactions with time weighting.\n",
    "    \n",
    "    Args:\n",
    "        behavior_data: DataFrame with user_id, history, and optionally timestamp columns\n",
    "        use_timestamps: Whether to use timestamp information for weighting\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with user_id, article_id, and weight columns\n",
    "    \"\"\"\n",
    "    print(\"Processing interactions...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Initialize list to store interactions\n",
    "    interactions = []\n",
    "    \n",
    "    # Check if timestamp column exists\n",
    "    has_timestamp = 'timestamp' in behavior_data.columns and use_timestamps\n",
    "    \n",
    "    if has_timestamp:\n",
    "        # Convert timestamp to datetime if not already\n",
    "        if not pd.api.types.is_datetime64_any_dtype(behavior_data['timestamp']):\n",
    "            behavior_data['timestamp'] = pd.to_datetime(behavior_data['timestamp'], errors='coerce')\n",
    "        \n",
    "        # Get reference timestamp (max timestamp in the dataset)\n",
    "        reference_time = behavior_data['timestamp'].max()\n",
    "    \n",
    "    # Process each row\n",
    "    for idx, row in tqdm(behavior_data.iterrows(), total=len(behavior_data), desc=\"Extracting interactions\"):\n",
    "        user_id = row['user_id']\n",
    "        \n",
    "        if pd.isna(row['history']) or row['history'].strip() == \"\":\n",
    "            continue\n",
    "        \n",
    "        history = row['history'].split()\n",
    "        \n",
    "        # Apply time decay if timestamps available\n",
    "        time_weight = 1.0\n",
    "        if has_timestamp and pd.notna(row['timestamp']):\n",
    "            # Calculate days from reference time\n",
    "            days_diff = (reference_time - row['timestamp']).total_seconds() / (24 * 3600)\n",
    "            # Apply exponential decay (adjust the 0.1 factor to control decay rate)\n",
    "            time_weight = np.exp(-0.1 * days_diff)\n",
    "        \n",
    "        # Process each article in user history\n",
    "        for position, article_id in enumerate(history):\n",
    "            # Position weight (more recent items get higher weight)\n",
    "            position_weight = 1.0 - 0.8 * (len(history) - 1 - position) / max(1, len(history) - 1)\n",
    "            \n",
    "            # Combine weights\n",
    "            weight = position_weight * time_weight\n",
    "            \n",
    "            interactions.append({\n",
    "                'user_id': user_id,\n",
    "                'article_id': article_id,\n",
    "                'weight': weight\n",
    "            })\n",
    "    \n",
    "    # Create DataFrame\n",
    "    interactions_df = pd.DataFrame(interactions)\n",
    "    \n",
    "    # Aggregate weights for duplicate user-item pairs\n",
    "    if not interactions_df.empty:\n",
    "        interactions_df = interactions_df.groupby(['user_id', 'article_id'])['weight'].sum().reset_index()\n",
    "    \n",
    "    end_time = time.time()\n",
    "    print(f\"Processed {len(interactions_df)} interactions in {end_time - start_time:.2f} seconds\")\n",
    "    \n",
    "    return interactions_df\n",
    "\n",
    "def create_matrices(interactions_df, max_items=None):\n",
    "    \"\"\"\n",
    "    Create user-item and item-item matrices for collaborative filtering.\n",
    "    \n",
    "    Args:\n",
    "        interactions_df: DataFrame with user_id, article_id, and weight columns\n",
    "        max_items: Maximum number of items to keep (most popular)\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of matrices and mappings\n",
    "    \"\"\"\n",
    "    print(\"Creating matrices...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Limit to most popular items if needed\n",
    "    if max_items and interactions_df['article_id'].nunique() > max_items:\n",
    "        # Get item popularity\n",
    "        item_popularity = interactions_df.groupby('article_id')['weight'].sum().reset_index()\n",
    "        # Sort by weight and keep top items\n",
    "        top_items = item_popularity.sort_values('weight', ascending=False).head(max_items)['article_id'].values\n",
    "        # Filter interactions\n",
    "        interactions_df = interactions_df[interactions_df['article_id'].isin(top_items)]\n",
    "        print(f\"Filtered to top {max_items} items\")\n",
    "    \n",
    "    # Create mappings from IDs to indices\n",
    "    user_ids = interactions_df['user_id'].unique()\n",
    "    article_ids = interactions_df['article_id'].unique()\n",
    "    \n",
    "    user_id_to_idx = {id: i for i, id in enumerate(user_ids)}\n",
    "    article_id_to_idx = {id: i for i, id in enumerate(article_ids)}\n",
    "    \n",
    "    # Map IDs to indices\n",
    "    user_indices = interactions_df['user_id'].map(user_id_to_idx).values\n",
    "    article_indices = interactions_df['article_id'].map(article_id_to_idx).values\n",
    "    \n",
    "    # Use weights from interactions\n",
    "    interaction_values = interactions_df['weight'].values\n",
    "    \n",
    "    # Create the sparse user-item matrix\n",
    "    user_item_matrix = csr_matrix(\n",
    "        (interaction_values, (user_indices, article_indices)),\n",
    "        shape=(len(user_ids), len(article_ids))\n",
    "    )\n",
    "    \n",
    "    # Calculate item popularity (for cold-start cases)\n",
    "    item_popularity = np.array(user_item_matrix.sum(axis=0)).flatten()\n",
    "    item_popularity = item_popularity / np.sum(item_popularity)\n",
    "    \n",
    "    # Create reverse mappings\n",
    "    idx_to_user_id = {i: id for id, i in user_id_to_idx.items()}\n",
    "    idx_to_article_id = {i: id for id, i in article_id_to_idx.items()}\n",
    "    \n",
    "    end_time = time.time()\n",
    "    print(f\"Matrix creation completed in {end_time - start_time:.2f} seconds\")\n",
    "    \n",
    "    return (user_item_matrix, \n",
    "            user_id_to_idx, article_id_to_idx, \n",
    "            idx_to_user_id, idx_to_article_id,\n",
    "            item_popularity)\n",
    "\n",
    "def create_item_similarity_matrix(user_item_matrix, batch_size=1000):\n",
    "    \"\"\"\n",
    "    Create item similarity matrix using normalized cosine similarity.\n",
    "    Processes in batches for memory efficiency.\n",
    "    \n",
    "    Args:\n",
    "        user_item_matrix: Sparse user-item matrix\n",
    "        batch_size: Size of batches to process\n",
    "    \n",
    "    Returns:\n",
    "        Item similarity matrix (sparse)\n",
    "    \"\"\"\n",
    "    print(\"Computing item similarity matrix...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Transpose to get item-user matrix\n",
    "    item_user_matrix = user_item_matrix.T.tocsr()\n",
    "    \n",
    "    # Normalize the item vectors for better similarity\n",
    "    normalized_item_matrix = normalize(item_user_matrix, norm='l2', axis=1)\n",
    "    \n",
    "    # Get dimensions\n",
    "    n_items = normalized_item_matrix.shape[0]\n",
    "    \n",
    "    # Initialize similarity matrix\n",
    "    item_similarities = csr_matrix((n_items, n_items))\n",
    "    \n",
    "    # Process in batches\n",
    "    for start_idx in range(0, n_items, batch_size):\n",
    "        end_idx = min(start_idx + batch_size, n_items)\n",
    "        current_batch = normalized_item_matrix[start_idx:end_idx]\n",
    "        \n",
    "        # Compute similarity between current batch and all items\n",
    "        batch_similarity = cosine_similarity(\n",
    "            current_batch, \n",
    "            normalized_item_matrix,\n",
    "            dense_output=False\n",
    "        )\n",
    "        \n",
    "        # Set self-similarity to zero (important for recommendation logic)\n",
    "        for i in range(end_idx - start_idx):\n",
    "            batch_similarity[i, start_idx + i] = 0\n",
    "        \n",
    "        # Add to similarity matrix\n",
    "        item_similarities = item_similarities + csr_matrix(\n",
    "            (batch_similarity.data, \n",
    "             (batch_similarity.row + start_idx, batch_similarity.col)),\n",
    "            shape=(n_items, n_items)\n",
    "        )\n",
    "        \n",
    "        print(f\"Processed batch {start_idx//batch_size + 1}/{(n_items + batch_size - 1)//batch_size}\")\n",
    "    \n",
    "    end_time = time.time()\n",
    "    print(f\"Similarity matrix computation completed in {end_time - start_time:.2f} seconds\")\n",
    "    \n",
    "    return item_similarities\n",
    "\n",
    "def generate_predictions(test_behavior_data,\n",
    "                        user_item_matrix,\n",
    "                        item_similarities,\n",
    "                        user_id_to_idx,\n",
    "                        article_id_to_idx,\n",
    "                        idx_to_article_id,\n",
    "                        item_popularity):\n",
    "    \"\"\"\n",
    "    Generate predictions optimized for MIND evaluation metrics.\n",
    "    \n",
    "    Args:\n",
    "        test_behavior_data: DataFrame containing test data\n",
    "        user_item_matrix: Sparse user-item matrix\n",
    "        item_similarities: Sparse item-item similarity matrix\n",
    "        user_id_to_idx: Mapping from user IDs to matrix indices\n",
    "        article_id_to_idx: Mapping from article IDs to matrix indices\n",
    "        idx_to_article_id: Mapping from matrix indices to article IDs\n",
    "        item_popularity: Array of item popularity scores\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with impression_id, news_id, and score columns\n",
    "    \"\"\"\n",
    "    print(\"Generating predictions...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    predictions = []\n",
    "    \n",
    "    # Process each row in test data\n",
    "    for idx, row in tqdm(test_behavior_data.iterrows(), \n",
    "                        total=len(test_behavior_data), \n",
    "                        desc=\"Processing test data\"):\n",
    "        impression_id = row.get('impression_id', idx)\n",
    "        user_id = row['user_id']\n",
    "        \n",
    "        if pd.isna(row['impressions']):\n",
    "            continue\n",
    "        \n",
    "        # Parse impression articles\n",
    "        impression_articles = []\n",
    "        for imp in row['impressions'].split():\n",
    "            parts = imp.split('-')\n",
    "            article_id = parts[0]\n",
    "            impression_articles.append(article_id)\n",
    "        \n",
    "        # Get user's history\n",
    "        user_history = []\n",
    "        if pd.notna(row['history']) and row['history'].strip() != \"\":\n",
    "            user_history = row['history'].split()\n",
    "        \n",
    "        # Generate scores for each impression article\n",
    "        for article_id in impression_articles:\n",
    "            # Default score\n",
    "            score = 0.5\n",
    "            \n",
    "            if article_id in article_id_to_idx:\n",
    "                article_idx = article_id_to_idx[article_id]\n",
    "                \n",
    "                # Use different strategies based on whether we know this user\n",
    "                if user_id in user_id_to_idx:\n",
    "                    user_idx = user_id_to_idx[user_id]\n",
    "                    \n",
    "                    # Personalized recommendation approach\n",
    "                    # 1. Direct user-item interaction term\n",
    "                    if user_item_matrix[user_idx, article_idx] > 0:\n",
    "                        # User has already interacted with this article\n",
    "                        score = 0.3  # Lower score for already read articles\n",
    "                    else:\n",
    "                        # Calculate similarity-based score\n",
    "                        # Convert user history to indices\n",
    "                        history_indices = []\n",
    "                        for hist_id in user_history:\n",
    "                            if hist_id in article_id_to_idx:\n",
    "                                history_indices.append(article_id_to_idx[hist_id])\n",
    "                        \n",
    "                        if history_indices:\n",
    "                            # Calculate total similarity to history articles\n",
    "                            sim_scores = []\n",
    "                            \n",
    "                            for hist_idx in history_indices:\n",
    "                                # Get similarity between history item and candidate\n",
    "                                sim = item_similarities[hist_idx, article_idx]\n",
    "                                if sim > 0:\n",
    "                                    sim_scores.append(sim)\n",
    "                            \n",
    "                            if sim_scores:\n",
    "                                # Use mean of top similarities for score\n",
    "                                # Using top similarities helps focus on most relevant patterns\n",
    "                                top_sims = sorted(sim_scores, reverse=True)[:3]\n",
    "                                cf_score = np.mean(top_sims)\n",
    "                                \n",
    "                                # Add popularity component\n",
    "                                pop_score = item_popularity[article_idx]\n",
    "                                \n",
    "                                # Combine (higher weight to CF score)\n",
    "                                score = 0.8 * cf_score + 0.2 * pop_score\n",
    "                            else:\n",
    "                                # Fall back to popularity-based score\n",
    "                                score = 0.3 + 0.5 * item_popularity[article_idx]\n",
    "                        else:\n",
    "                            # No matching history - use popularity\n",
    "                            score = 0.3 + 0.5 * item_popularity[article_idx]\n",
    "                else:\n",
    "                    # Cold-start user - use popularity with a dampening factor\n",
    "                    # This prevents overweighting very popular items\n",
    "                    popularity = item_popularity[article_idx]\n",
    "                    score = 0.3 + 0.7 * (popularity ** 0.5)\n",
    "            \n",
    "            # Apply sigmoid transformation to sharpen the scoring distribution\n",
    "            # This helps improve AUC by creating more separation between scores\n",
    "            score = 1.0 / (1.0 + np.exp(-5 * (score - 0.5)))\n",
    "            \n",
    "            # Add prediction\n",
    "            predictions.append({\n",
    "                'impression_id': impression_id,\n",
    "                'news_id': article_id,\n",
    "                'score': float(score)\n",
    "            })\n",
    "    \n",
    "    # Create DataFrame\n",
    "    predictions_df = pd.DataFrame(predictions)\n",
    "    \n",
    "    # Convert datatypes for evaluation\n",
    "    try:\n",
    "        predictions_df['impression_id'] = predictions_df['impression_id'].astype('int64')\n",
    "    except:\n",
    "        predictions_df['impression_id'] = predictions_df['impression_id'].astype(str)\n",
    "    \n",
    "    predictions_df['news_id'] = predictions_df['news_id'].astype(str)\n",
    "    predictions_df['score'] = predictions_df['score'].astype(float)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    print(f\"Prediction generation completed in {end_time - start_time:.2f} seconds\")\n",
    "    \n",
    "    return predictions_df\n",
    "\n",
    "def mind_optimized_recommender(behavior_data, test_behavior_data, max_items=50000, batch_size=1000):\n",
    "    \"\"\"\n",
    "    Complete pipeline for MIND-optimized hybrid recommender.\n",
    "    \n",
    "    Args:\n",
    "        behavior_data: DataFrame containing training data\n",
    "        test_behavior_data: DataFrame containing test data\n",
    "        max_items: Maximum number of items to keep\n",
    "        batch_size: Batch size for similarity computation\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with impression_id, news_id, and score columns\n",
    "    \"\"\"\n",
    "    print(\"Starting MIND-optimized recommender...\")\n",
    "    overall_start = time.time()\n",
    "    \n",
    "    # Process interactions\n",
    "    interactions_df = process_interactions(behavior_data)\n",
    "    \n",
    "    # Create matrices\n",
    "    (user_item_matrix, \n",
    "     user_id_to_idx, article_id_to_idx, \n",
    "     idx_to_user_id, idx_to_article_id,\n",
    "     item_popularity) = create_matrices(\n",
    "        interactions_df,\n",
    "        max_items=max_items\n",
    "    )\n",
    "    \n",
    "    # Create item similarity matrix\n",
    "    item_similarities = create_item_similarity_matrix(\n",
    "        user_item_matrix,\n",
    "        batch_size=batch_size\n",
    "    )\n",
    "    \n",
    "    # Generate predictions\n",
    "    predictions_df = generate_predictions(\n",
    "        test_behavior_data,\n",
    "        user_item_matrix,\n",
    "        item_similarities,\n",
    "        user_id_to_idx,\n",
    "        article_id_to_idx,\n",
    "        idx_to_article_id,\n",
    "        item_popularity\n",
    "    )\n",
    "    \n",
    "    overall_end = time.time()\n",
    "    print(f\"Pipeline completed in {overall_end - overall_start:.2f} seconds\")\n",
    "    print(f\"Generated {len(predictions_df)} predictions\")\n",
    "    \n",
    "    return predictions_df\n",
    "\n",
    "# Example usage:\n",
    "predictions_df = mind_optimized_recommender(\n",
    "     behavior_data, \n",
    "     test_behavior_data,\n",
    "    max_items=50000,\n",
    "    batch_size=1000\n",
    " )\n",
    "# predictions_df.to_csv('mind_optimized_predictions.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lib.eval as eval\n",
    "import importlib\n",
    "\n",
    "importlib.reload(eval)\n",
    "\n",
    "eval.evaluate_mind_predictions(predictions_df, test_behavior_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
